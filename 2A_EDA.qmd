---
title: "Exploratory Data Analysis"
author: "Juan Fonseca"
format: html
---

```{r libraries, message=FALSE,warning=FALSE}
library(tidyverse)
library(kableExtra)
library(hrbrthemes)
library(tmap)
library(sf)
```

```{r data_load}
net_data_sf <- sf::st_read("03_preprocessing_files/network_data.gpkg", layer = "network_data")
LSOA_sf <- sf::st_read("03_preprocessing_files/network_data.gpkg", layer = "LSOA")

jct_sf <- sf::st_read("03_preprocessing_files/network_data.gpkg", layer = "junctions")

LSOA_data <- LSOA_sf |> sf::st_drop_geometry()
```

LSOA data is joined with the network data. Also, total population and total
people employed are scaled based on the length proportion calculated before. Road type is converted to factor

```{r subset_data}
model_data_sf <- net_data_sf |> left_join(LSOA_data |>
                                            select(LSOA21CD,
                                                   total_pop,
                                                   total_employed,
                                                   wk_pop,
                                                   car_avail_perc,
                                                   road_density,
                                                   cars_percap_2018,area_km2) |> 
                                            rename(total_emp_pop = total_employed),
                                          by = join_by(LSOA21CD)) |> 
  mutate(across(total_pop:wk_pop,
                \(x) x*portion_lsoa))
```


```{r drop_geom}
model_data <- model_data_sf |>
  sf::st_drop_geometry() 
```

<!-- include a quick analysis of the monitored roads minor vs major, also junctions -->
Extracting sample size based on the number of monitored edges
```{r table_counts}
model_data |>
  mutate(bool.flow.2023 = !is.na(flow.2023)) |> 
  summarise(n_edges = n(),.by = c(bool.flow.2023,road_type)) |>
  mutate(sample_size = n_edges/sum(n_edges),
         .by = road_type) |> 
  filter(bool.flow.2023) |> 
  select(road_type,sample_size) |> 
  kable(digits = 3) |> 
  kable_minimal()
```

Same analysis with distance (%)
```{r}
model_data |>
  mutate(bool.flow.2023 = !is.na(flow.2023)) |> 
  summarise(d_edges = sum(d),.by = c(bool.flow.2023,road_type)) |>
  mutate(sample_size = d_edges/sum(d_edges),
         .by = road_type) |> 
  filter(bool.flow.2023) |> 
  select(road_type,sample_size) |> 
  kable(digits = 3) |> 
  kable_minimal()
```

Analysis of junction types with flows
```{r}
jct_sf|>
  sf::st_drop_geometry() |> 
  mutate(bool.flow.2023 = !is.na(flow.2023)) |> 
  summarise(n_jct = n(),.by = c(bool.flow.2023,jct_type)) |>
  mutate(sample_size = n_jct/sum(n_jct),
         .by = jct_type) |> 
  filter(bool.flow.2023) |> 
  select(jct_type,sample_size) |> 
  kable(digits = 3) |> 
  kable_minimal()
```


Centrality vs Flows

```{r}
model_data |> 
  ggplot(aes(x=std.centrality,y=flow.2023,col=road_type))+
  geom_point()+
  geom_smooth(method = "lm",se = F)+
  theme_ipsum_rc()
```

Pop with car availability vs flow
```{r}
model_data |> 
  ggplot(aes(x=(total_pop*car_avail_perc),
             y=flow.2023,
             col=road_type))+
  geom_point()+
  geom_smooth(method = "lm",se = F)+
  theme_ipsum_rc()
```

Employed Pop with car availability vs flow
```{r}
model_data |> 
  ggplot(aes(x=(total_emp_pop*car_avail_perc),
             y=flow.2023,
             col=road_type))+
  geom_point()+
  geom_smooth(method = "lm",se = F)+
  theme_ipsum_rc()
```

Workplace Pop vs flow
```{r}
model_data |> 
  ggplot(aes(x=wk_pop,
             y=flow.2023,
             col=road_type))+
  geom_point()+
  scale_x_log10()+
  geom_smooth(method = "lm",se = F)+
  theme_ipsum_rc()
```



Alternatively, population and jobs  serviced can be used as as per Selby 2011.

```{r}
isochrones <- read_csv("03_preprocessing_files/junctions_catchment.csv")
```


```{r}
consolidated_catchment <- isochrones |>
  mutate(id = as.character(id)) |> 
  left_join(model_data |>
              select(to_id, total_pop, total_emp_pop, wk_pop),
            by=c("id"="to_id"),
            relationship = "many-to-many") |> 
  summarise(across(ends_with("pop"),
                   \(x) sum(x,na.rm = T)),
            .by = c(from,tlim)) |>
  right_join(isochrones |> 
                 expand(from,tlim),
             by = c("from","tlim")) |> 
  arrange(from,tlim) |> 
  mutate(across(ends_with("pop"),
                   \(x) if_else(is.na(x),0,x)),
         across(ends_with("pop"),
                   list(c = \(x) cumsum(x))),
            .by = from) |> 
  select(from,tlim,ends_with("c")) |> 
  pivot_wider(names_from = tlim,values_from = total_pop_c:wk_pop_c) |> 
  mutate(from = as.character(from))

```


Catchment of residentes within 2 minutes
```{r}
tm_shape(model_data_sf)+
  tm_lines("skyblue",lwd = 0.7)+
  tm_shape(model_data_sf |>
  left_join(consolidated_catchment,
            by = c("from_id"="from")) |> 
  filter(!is.na(total_pop_c_2)))+
  tm_lines("total_pop_c_2",lwd = 2)
```

Catchment of employed residents within 2 minutes
```{r}
tm_shape(model_data_sf)+
  tm_lines("skyblue",lwd = 0.7)+
  tm_shape(model_data_sf |>
  left_join(consolidated_catchment,
            by = c("from_id"="from")) |> 
  filter(!is.na(total_emp_pop_c_2)))+
  tm_lines("wk_pop_c_2",lwd = 2)
```

Catchment of usual workplace population
```{r}
tm_shape(model_data_sf)+
  tm_lines("skyblue",lwd = 0.7)+
  tm_shape(model_data_sf |>
  left_join(consolidated_catchment,
            by = c("from_id"="from")) |> 
  filter(!is.na(wk_pop_c_2)))+
  tm_lines("wk_pop_c_2",lwd = 2)
```

Joining the catchment to the `model_data` dataset

```{r}
model_data_expanded <- model_data_sf |>
  left_join(consolidated_catchment,
            by = c("from_id"="from")) |>
  select(edge_id,
         flow.2022,
         flow.2023,
         centrality,
         car_avail_perc,
         total_pop_c_1:wk_pop_c_5,
         road_density) |>
  mutate(across(contains("pop"),\(x) x/1e3),
         centrality = centrality/1e6,
         across(starts_with("flow."),\(x) round(x) |> as.integer()))
```


```{r}
model_data_expanded |>
  st_drop_geometry() |> 
  select(flow.2023, centrality, total_pop_c_2,total_emp_pop_c_2, wk_pop_c_2) |>
  drop_na(flow.2023, centrality, total_pop_c_2,total_emp_pop_c_2, wk_pop_c_2) |>
  # mutate(across(everything(),log)) |> 
  GGally::ggpairs()
```


## GLM model

```{r}
model_data_noNAs<- model_data_expanded |> 
  select(flow.2022,
         flow.2023,
         centrality,
         car_avail_perc,
         total_pop_c_1:wk_pop_c_5,
         road_density) |>
  drop_na()
```

A simple model with the centrality only for major roads

```{r}
GLM_models <- lapply(
  1:5,
  function(i) {
    myformula <- as.formula(paste0(
      "flow.2023 ~ total_pop_c_",
      i,
      " + wk_pop_c_",
      i,
      " + road_density"
    ))
    
    m <- glm(myformula,
             data = model_data_noNAs,
             family = poisson())
  }
)
```


```{r}
lapply(GLM_models,
       function(mymodel){
         beta <- coef(mymodel)
         exp(confint(mymodel,level = 0.95))
       })
```

RMSE
```{r}
GLM_RMSE <- vapply(
  GLM_models,
  function(mymodel) {
    mean((mymodel$data$flow.2023 - mymodel$fitted.values) ^ 2) ^ 0.5
  },
  numeric(1))
```

AICc

null model to compare AICc of fitted models
```{r}
null.m <- glm(flow.2023 ~ 1,
              data = model_data_noNAs,
              family = poisson())
# AICc for null model
extractAIC(null.m)[2]
```


```{r}
GLM_AICc <- vapply(GLM_models,
                   function(mymodel) {
                     extractAIC(mymodel)[2]
                   },
                   numeric(1)) 
```

## GWPR

```{r}
library(GWmodel)

model_data_noNAs_sp <- as(model_data_noNAs |> st_centroid(),"Spatial")
dMat <- model_data_noNAs |> st_centroid() |> st_coordinates() |> gw.dist()

GWPR_models <- lapply(
  1:5,
  function(i){
    myformula <- as.formula(paste0(
      "flow.2023 ~ total_pop_c_",
      i,
      " + wk_pop_c_",
      i,
      " + road_density"
    ))
    
    # Determine the adaptive bandwidth
    abw <- bw.ggwr(
      formula = myformula,
      data = model_data_noNAs_sp,
      family = "poisson",
      approach = "AICc",
      kernel = "bisquare",
      adaptive = TRUE,
      dMat = dMat
    )
    # Fit GWPR
    gwpr.m <- ggwr.basic(
      myformula,
      data = model_data_noNAs_sp,
      family = "poisson",
      bw = abw,
      kernel = "bisquare",
      adaptive = TRUE,
      dMat = dMat
    )
    }
  )
```


```{r}
lapply(GWPR_models,
       function(mymodel){
         # table of GWR coefficients
         t1 = exp(apply(mymodel$SDF@data[, 1:4], 2, summary)) |> t()
       })
```


RMSE
```{r}
GWPR_RMSE <- vapply(
  GWPR_models,
  function(mymodel) {
    mean((mymodel$SDF$y - mymodel$SDF$yhat) ^ 2) ^ 0.5
  },
  numeric(1))
```


AICc
```{r}
GWPR_AICc <- vapply(GWPR_models,
                   function(mymodel) {
                     mymodel$glms$aic
                   },
                   numeric(1)) 
```








```{r}
pred.flow.2023 <- predict(m1,model_data_expanded)

wide_net_data <- model_data_sf |> cbind(pred.flow.2023) |> 
  mutate(residuals = pred.flow.2023-flow.2023)
```

```{r}
tmap_mode("plot")
tm_shape(wide_net_data)+
  tm_lines("pred.flow.2023",palette = "magma",style = "fisher")
```

```{r}
wide_net_data |>
  filter(is.na(residuals)) |> 
tm_shape()+
  tm_lines("grey", lwd = 0.5,midpoint = 0) +  
tm_shape(wide_net_data |>
  filter(!is.na(residuals)))+
  tm_lines("residuals",palette = "viridis",style = "fisher",lwd = 2,midpoint = 0)
```



This to do:

 -  add a function to run the regression models with different catchment distances done
 -  extract RMSE done, AICc done and $R^2$ metrics
 - add a GLM of the poisson family done
 - add GWPR done
 - add a bayesian linear regression
 
 


<!-- short links with flows but low centrality -->
<!-- a simplified network would reduce the noise -->













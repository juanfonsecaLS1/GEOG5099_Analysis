[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro",
    "section": "",
    "text": "This repository contains the code produced for a traffic estimation project for GEO5099."
  },
  {
    "objectID": "B1_Road_network.html",
    "href": "B1_Road_network.html",
    "title": "Road Network",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dodgr)\nlibrary(osmextract)\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\nCheck the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\nThe following code uses some functions compiled in the MinorRoadTraffic repository prepared by Malcolm Morgan and follows the process described in this vignette of the same package.\n# remotes::install_github(\"ITSLeeds/MinorRoadTraffic\") # if not installed\nlibrary(MinorRoadTraffic)"
  },
  {
    "objectID": "B1_Road_network.html#downloading-data",
    "href": "B1_Road_network.html#downloading-data",
    "title": "Road Network",
    "section": "Downloading data",
    "text": "Downloading data\n\nosm_raw &lt;- oe_get(\"East Yorkshire with Hull\",\n                 extra_tags = c(\"ref\", \"highway\", \"junction\", \"maxspeed\"))\n\nThe input place was matched with: East Yorkshire with Hull\n\n\nThe chosen file was already detected in the download directory. Skip downloading.\n\n\nThe corresponding gpkg file was already detected. Skip vectortranslate operations.\n\n\nReading layer `lines' from data source \n  `C:\\Users\\ts18jpf\\Documents\\OSMEXT_downloads\\geofabrik_east-yorkshire-with-hull-latest.gpkg' \n  using driver `GPKG'\nSimple feature collection with 77201 features and 12 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -4.03885 ymin: 51.265 xmax: 4.364931 ymax: 58.67265\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "B1_Road_network.html#clipping-road-network",
    "href": "B1_Road_network.html#clipping-road-network",
    "title": "Road Network",
    "section": "Clipping road network",
    "text": "Clipping road network\nThe count data will be used to clip the road network.\n\nsf_counts &lt;- st_read(\"03_preprocessing_files/grouped_counts.geojson\")\n\nReading layer `grouped_counts' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\grouped_counts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 588 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 505930.6 ymin: 428325.3 xmax: 513352.9 ymax: 433636.7\nProjected CRS: OSGB36 / British National Grid\n\n\n\nbuffer_distance = max(st_distance(sf_counts,sf_counts))*0.15\n\nbounds &lt;- sf_counts |&gt;\n  make_convex_hull(dist = buffer_distance)\n\nosm_clean = extract_from_osm(osm_raw, bounds)\n\nnetwork = osm_clean$network\njunctions = osm_clean$junctions\nrm(osm_clean, osm_raw)\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_basemap()+\ntm_shape(bounds)+\n  tm_polygons(alpha = 0.3)+\n  tm_shape(network)+\n  tm_lines(col = \"orange\",alpha = 0.4)+\n  tm_shape(sf_counts)+\n  tm_dots(col = \"blue\")"
  },
  {
    "objectID": "B1_Road_network.html#cleaning-and-simplifying",
    "href": "B1_Road_network.html#cleaning-and-simplifying",
    "title": "Road Network",
    "section": "Cleaning and simplifying",
    "text": "Cleaning and simplifying\nThe ref attribute is filled based on the neighbouring links.\n\nsource(\"osm_fill_ref2.R\")\nnetwork_fill2 = osm_fill_ref2(network)\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5"
  },
  {
    "objectID": "B1_Road_network.html#spatial-join-to-count-data",
    "href": "B1_Road_network.html#spatial-join-to-count-data",
    "title": "Road Network",
    "section": "Spatial join to count data",
    "text": "Spatial join to count data"
  },
  {
    "objectID": "A1_Count_data.html",
    "href": "A1_Count_data.html",
    "title": "Count Data",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rjson)\nlibrary(httr)\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n\n\nSensor summary\nThe following code loads the file with the summary of scoot loop sensors in Hull\n\nCam_Locations  &lt;- GET(\"https://opendata.hullcc.gov.uk/dataset/30fd3969-556d-4eae-ae4c-f3f9d2cfa9e3/resource/90e1cce0-295e-4fa7-aa21-ebc4f3e8e8d4/download/scoot_loop_resources_full.json\")\n\nmy_response &lt;- rjson::fromJSON(content(Cam_Locations,'text',encoding = \"UTF-8\"))\n\nmy_data &lt;- data.frame(do.call(rbind,\n                             lapply(my_response,\n                                    rbind))) |&gt;\n  unnest(cols = everything()) |&gt;\n  filter(longitude != 0)\n\nA spatial object is created using the coordinates of the sites\n\nsf_cameras &lt;- my_data |&gt; \n  st_as_sf(\n    coords = c(\"longitude\",\"latitude\"),\n    crs = 4326) |&gt;\n  st_transform(crs = 27700)\n\n\ntm_basemap() +\n        tm_shape(sf_cameras) +\n        tm_dots()\n\n\n\n\nThere are some issues with the coordinates of some sites. The following code fixes the problem\n\nmy_data[my_data$latitude&lt;0,c(\"longitude\",\"latitude\")] &lt;- rev(my_data[my_data$latitude&lt;0,c(\"longitude\",\"latitude\")])\n\nThe description column contain useful information on the location of each sensor, this can be used to identify sensors corresponding to different lanes in the same road, for example.\n\nmy_data_expanded &lt;- my_data |&gt;\n  mutate(description = str_replace(description,\"HE SITE - \",\"HE SITE _ \")) |&gt; \n  separate_wider_delim(description,delim = \" - \",names = c(\"desc\",\"direction\",\"url\",\"coord\")) |&gt;\n  select(-coord)\n\n\nsf_sensors_raw &lt;- my_data_expanded |&gt; \n  st_as_sf(\n    coords = c(\"longitude\",\"latitude\"),\n    crs = 4326) |&gt;\n  st_transform(crs = 27700)\n\ntm_basemap() +\n        tm_shape(sf_sensors_raw) +\n        tm_dots()\n\n\n\n\n\nGrouping sensors\n\nsensors_buffer_5 = sf_sensors_raw |&gt; st_buffer(dist = 5)\n\n\ntm_basemap() +\n  tm_shape(sensors_buffer_5 |&gt; \n             filter(name %in% c(\"N44131F\",\"N44131X\",\"N44131H\"))) +\n        tm_polygons(alpha = 0.4)+\n        tm_shape(sf_sensors_raw) +\n        tm_dots()\n\n\n\n\n\nsensors_overlap = st_intersects(sensors_buffer_5,sensors_buffer_5) |&gt; unique()\nhead(sensors_overlap)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2 3\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 5\n\n[[5]]\n[1] 6\n\n[[6]]\n[1]  7 23\n\n\n\ndir_groups &lt;- do.call(bind_rows,\n                      lapply(seq_along(sensors_overlap),\n                             function(i){\n                               tsensor &lt;-  sensors_overlap[[i]]\n                               id_group  &lt;- i\n                               tmp_group &lt;- my_data_expanded[tsensor,] |&gt;\n                                 mutate(subgroup_id = cur_group_id(),\n                                        group_id = id_group,\n                                        .by = c(desc,direction)) |&gt;\n                                 select(name,desc,direction,group_id,subgroup_id)\n                               })) \n\n\nsf_counts &lt;- my_data_expanded |&gt; \n  left_join(dir_groups,by = join_by(name,desc,direction)) |&gt; \n  summarise(across(ends_with(\"itude\"),mean),\n            .by = c(group_id,subgroup_id,desc,direction)) |&gt; \n  st_as_sf(\n    coords = c(\"longitude\",\"latitude\"),\n    crs = 4326) |&gt;\n  st_transform(crs = 27700)\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap()+\n  tm_shape(sf_counts) +\n  tm_dots()\n\n\n\n\n\n\n\nDownloading data\nSince the open data platform hosting the counts data is based on CKAN, the raw CSV files can be downloaded directly with the following code (see this):\n\ndir.create(\"02_raw_data_counts\", showWarnings = F)\n\nbase_url &lt;- \"https://opendata.hullcc.gov.uk/datastore/dump/\"\nbase_path &lt;- \"02_raw_data_counts\"\n\n\n# In case files are downloaded separately \ndownloaded_files &lt;- list.files(\"02_raw_data_counts/\")\nids_download &lt;- my_data$resource_id[!(my_data$name %in% gsub(\"\\\\.csv\",\"\",x = downloaded_files))]\n\n# Loop for downloading files\nif (!identical(ids_download, character(0))) {\n  for (id in ids_download) {\n    try(download.file(\n      url = paste0(base_url, id),\n      destfile = paste0(base_path,\n                        \"/\",\n                        my_data$name[my_data$resource_id == id],\n                        \".csv\")\n    ))\n  }\n} \n\n\n\nPre-processing\nFirst, we upload the downloaded_files object with all the files available\n\ndownloaded_files_full &lt;- list.files(\"02_raw_data_counts/\",full.names = T)\n\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(dtplyr)\nsetDTthreads(0)\n\nraw_data &lt;- rbindlist(lapply(downloaded_files_full,fread))\n\nWarning in FUN(X[[i]], ...): Discarded single-line footer: &lt;&lt;3225&gt;&gt;\n\nhead(raw_data)\n\n   _id  LinkID AttributeID TravelTime VehicleFlow Speed     MeasurementTime\n1:   1 N10111A     9601773          0           0     0 2020-02-07 15:38:00\n2:   2 N10111A     9601773          0           0    80 2020-02-07 16:49:14\n3:   3 N10111A     9601773          0           0    80 2020-02-07 16:59:14\n4:   4 N10111A     9601773          0           0    80 2020-02-07 17:04:14\n5:   5 N10111A     9601773          0           0    80 2020-02-07 17:09:14\n6:   6 N10111A     9601773         70           9    80 2020-02-07 17:19:14\n    Timestamp\n1: 1581089880\n2: 1581094154\n3: 1581094754\n4: 1581095054\n5: 1581095354\n6: 1581095954\n\n\nSome general checks of the data\nNumber of sites in the raw data:\n\nraw_data$LinkID |&gt; unique() |&gt; length()\n\n[1] 296\n\n\nRange of dates:\n\nraw_data$MeasurementTime |&gt; range()\n\n[1] \"2020-02-07 15:38:00 UTC\" \"2024-01-09 19:30:00 UTC\"\n\n\n\nCollection rate check\nA check of the number of records per site\n\nhist(raw_data |&gt;\n       summarise(records = n(),.by = LinkID) |&gt;\n       as_tibble() |&gt;\n       pull(records),breaks = seq(0,350000,1000),\n     xlab = \"Number of records\",\n     main = \"Distribution of total records per LinkID\")\n\n\n\n\nA closer look to the sites with a low number of records:\n\nlow_records_IDs &lt;- raw_data |&gt;\n  filter(between(year(MeasurementTime),2022,2023)) |&gt; \n  summarise(records = n(),.by = LinkID) |&gt;\n  filter(records &lt; 150000) |&gt; \n  as_tibble() |&gt; \n  pull(LinkID)\n\nlow_records_IDs\n\n [1] \"N10221X\" \"N30121C\" \"N48111F\" \"N48111G\" \"N48112B\" \"N48112C\" \"N48112D\"\n [8] \"N48112E\" \"N70111A\" \"N70111B\" \"N70111C\" \"N70111D\" \"N70121F\" \"N70121H\"\n[15] \"N70131B\" \"N70131D\" \"N70141E\" \"N70141F\" \"N70141H\"\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_basemap()+\n  tm_shape(sf_sensors_raw |&gt;\n  filter(!(name %in% low_records_IDs)))+\n  tm_dots(col = \"black\",\n          alpha = 0.3,\n          group = \"Other sites\")+\n  tm_shape(sf_sensors_raw |&gt;\n             filter(name %in% low_records_IDs))+\n  tm_dots(col = \"red\",\n          group = \"Low record sites\")\n\n\n\n\n\n\n\ndaily_data &lt;- raw_data[between(year(MeasurementTime),2022,2023)\n             ][,GBtimestamp := with_tz(MeasurementTime,tzone = \"Europe/London\")\n               ][,`:=`(Date = date(GBtimestamp),\n                       Year = year(GBtimestamp))][\n                         ,.(records = .N,\n                            Flow = sum(VehicleFlow,na.rm = T)),\n                         by = .(LinkID, Date, Year)\n                       ]\n\nRecords per day per site by Year (max 288 5-minutes intervals)\n\ndaily_data[,.(records = mean(records)),\n  .(LinkID, Year)] |&gt; \nggplot(aes(records))+\n  geom_histogram(binwidth = 1,col = \"white\")+\n  facet_grid(Year~.)+\n  theme_light()\n\n\n\n\nTo identify the IDs with a low number of daily records, we run:\n\nlow_annual_IDs &lt;- unique(daily_data[,c(\"Year\",\"LinkID\",\"Date\")])[,\n                                               .(n_days = .N),\n                                               .(LinkID, Year)][n_days&lt;240] |&gt; pull(LinkID)\n\nThe IDs with low records match exactly the ones previously identified.\n\nidentical(low_annual_IDs,low_records_IDs)\n\n[1] TRUE\n\n\nThese IDs will be discarded as the available data might not be representative to produce AADF; if the ID is part of a group, the whole group will be discarded as AADF for the group can be affected.\n\ngroups_include &lt;- dir_groups |&gt; \n  mutate(not_include = (name %in% low_annual_IDs)*1) |&gt; \n  filter(sum(not_include)==0,.by = c(desc,direction,group_id)) |&gt;\n  select(-not_include) \n\n\n\nDaily flows sense check\n\ntotal_d_flow &lt;- daily_data |&gt; \n  inner_join(groups_include,by = c(\"LinkID\"=\"name\")) |&gt; \n  summarise(Flow = sum(Flow),.by=Date)\n\n\n  ggplot(total_d_flow,\n         aes(x=Date,y=Flow))+\n  geom_line() +\n  geom_point(data = total_d_flow |&gt;\n               filter (Flow&lt;500e3),\n             col = \"red\")+\n  geom_text(data = total_d_flow |&gt;\n               filter (Flow&lt;500e3),\n            aes(label = Date),\n             col = \"#020202\")\n\n\n\n\nThe daily flows on Christmas day are used a sensible threshold to identify outliers which will be discarded. It is assumed that the lowest traffic over the year occurs on that day.\n\nmin_Xmas &lt;- daily_data |&gt;\n  filter(day(Date)==25,month(Date)==12) |&gt; \n  summarise(Flow = sum(Flow),.by=c(Date)) |&gt; \n  pull(Flow) |&gt; \n  min()\n\ntotal_d_flow &lt;- daily_data |&gt; \n  inner_join(groups_include,by = c(\"LinkID\"=\"name\")) |&gt; \n  filter(sum(Flow)&gt;=(min_Xmas*0.99),.by=Date) |&gt; \n  summarise(Flow = sum(Flow),.by=Date)\n\n\n  ggplot(total_d_flow,\n         aes(x=Date,y=Flow))+\n  geom_line() +\n  geom_point(data = total_d_flow |&gt;\n               filter (Flow&lt;600e3),\n             col = \"red\")+\n  geom_text(data = total_d_flow |&gt;\n               filter (Flow&lt;600e3),\n            aes(label = Date),\n             col = \"#020202\")\n\n\n\n\n\n\nZero-flow sites check\n\nsite_d_flow &lt;- daily_data |&gt; \n  inner_join(groups_include,by = c(\"LinkID\"=\"name\")) |&gt; \n  filter(sum(Flow)&gt;=min_Xmas,.by=Date) |&gt; \n  summarise(Flow = sum(Flow),.by=c(Year,Date,group_id,subgroup_id)) \n\nno_flow_sites &lt;- site_d_flow |&gt;\n  summarise(Flow = sum(Flow),.by=c(group_id,subgroup_id)) |&gt;\n  filter(Flow == 0)\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_basemap()+\n  tm_shape(sf_counts |&gt;\n             anti_join(no_flow_sites,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"black\",alpha = 0.3,group = \"Other sites\")+\n  tm_shape(sf_counts |&gt;\n             semi_join(no_flow_sites,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"red\",group = \"Zero counts sites\")\n\n\n\n\n\n\n\n\nAnnual Average Daily Flow (AADF) calculation\n\naadf_data &lt;- site_d_flow |&gt; \n  anti_join(no_flow_sites,\n                       by = join_by(group_id,subgroup_id)) |&gt; \n  summarise(Flow = mean(Flow),.by = c(Year,group_id,subgroup_id)) |&gt; \n  pivot_wider(names_from = \"Year\",values_from = \"Flow\",names_prefix = \"flow.\")\n\n\n\n2022 vs 2023 flows high-level check\n\naadf_checks &lt;- aadf_data |&gt; \n  mutate(diff = flow.2023-flow.2022,\n         pdiff = diff/flow.2022) |&gt; \n  arrange(-abs(pdiff))\n\n\n  ggplot(data = aadf_checks,\n         aes(flow.2022,flow.2023))+\n  geom_smooth(formula = \"y ~ x+0\",method = \"lm\",se = F,alpha = 0.4)+\n  geom_point(shape = 19,alpha = 0.6)+\n  geom_point(data = aadf_checks |&gt;\n               filter(abs(pdiff)&gt;0.5|is.nan(pdiff)),\n             shape = 19,\n             alpha = 0.4,\n             size = 3,\n             col= \"red\")+\n  theme_light()+\n  coord_fixed()\n\n\n\n\n\nhigh_change_counts &lt;- aadf_checks |&gt;\n  filter(abs(pdiff)&gt;0.5|is.nan(pdiff))\n\n\ntm_basemap()+\n  tm_shape(sf_counts |&gt;\n             anti_join(high_change_counts,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"black\",alpha = 0.3,group = \"Other sites\")+\n  tm_shape(sf_counts |&gt;\n             semi_join(high_change_counts,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"red\",group = \"High change sites\")\n\n\n\n\n\n\nAlthough these changes seem suspicious, these records will not be discarded for the final analysis.\n\n\n\nSaving results\nThe spatial object with the counts is updated to be consistent with the AADF dataframe\n\nsf_counts_clean &lt;- sf_counts |&gt;\n  semi_join(aadf_data,\n            by = join_by(group_id,subgroup_id))\n\nThe following code produces a csv file with the AADF of all sites, and a geoJSON file for the .\n\ndir.create(\"03_preprocessing_files\",showWarnings = F)\nwrite_csv(aadf_data,file = \"03_preprocessing_files/aadf_data.csv\",append = F)\ntry(file.remove(\"03_preprocessing_files/grouped_counts.geojson\"))\n\nWarning in file.remove(\"03_preprocessing_files/grouped_counts.geojson\"): cannot\nremove file '03_preprocessing_files/grouped_counts.geojson', reason 'Permission\ndenied'\n\n\n[1] FALSE\n\nst_write(sf_counts_clean,\"03_preprocessing_files/grouped_counts.geojson\",append = F)\n\nWarning in CPL_write_ogr(obj, dsn, layer, driver,\nas.character(dataset_options), : GDAL Error 6: DeleteLayer() not supported by\nthis dataset.\n\n\nDeleting layer not supported by driver `GeoJSON'\nDeleting layer `grouped_counts' failed\nWriting layer `grouped_counts' to data source \n  `03_preprocessing_files/grouped_counts.geojson' using driver `GeoJSON'\nUpdating existing layer grouped_counts\nWriting 196 features with 4 fields and geometry type Point."
  },
  {
    "objectID": "C1_Census_data.html",
    "href": "C1_Census_data.html",
    "title": "Census data",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nlibrary(MinorRoadTraffic)\n\n\nbounds &lt;- st_read(dsn = \"03_preprocessing_files/bounds.geoJSON\")\n\nReading layer `bounds' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\bounds.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 504739.7 ymin: 427134.2 xmax: 514543.7 ymax: 434827.7\nProjected CRS: OSGB36 / British National Grid\n\ndownload_lsoa_2021 &lt;- function(url = \"https://github.com/juanfonsecaLS1/GEOG5099_Analysis/releases/download/v0/LSOA_Dec_2021_Boundaries_Generalised_Clipped_EW_BGC_2022_5605507071095448309.geojson\",\n                              bounds){\n  \ndir.create(file.path(tempdir(),\"lsoa\"))\nutils::download.file(url, destfile = file.path(tempdir(),\"lsoa\",\"LSOA_2021.geojson\"),\n                       mode = \"wb\")\n  res = sf::read_sf(file.path(tempdir(),\"lsoa\",\"LSOA_2021.geojson\"))\n  return(res)\n}\n\nlsoa &lt;- download_lsoa_2021()\n\nlsoa_fixed &lt;- lsoa |&gt; st_make_valid()\nlsoa_selected &lt;- lsoa_fixed[bounds,] |&gt; st_transform(4326)\nrm(lsoa,lsoa_fixed)\n\n\nlsoa_selected |&gt; \n  tm_shape()+\n  tm_polygons(col = \"blue\",\n              alpha = 0.3,\n              border.col = \"blue\")"
  },
  {
    "objectID": "C1_Census_data.html#lsoa-boundaries",
    "href": "C1_Census_data.html#lsoa-boundaries",
    "title": "Census data",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nlibrary(MinorRoadTraffic)\n\n\nbounds &lt;- st_read(dsn = \"03_preprocessing_files/bounds.geoJSON\")\n\nReading layer `bounds' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\bounds.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 504739.7 ymin: 427134.2 xmax: 514543.7 ymax: 434827.7\nProjected CRS: OSGB36 / British National Grid\n\ndownload_lsoa_2021 &lt;- function(url = \"https://github.com/juanfonsecaLS1/GEOG5099_Analysis/releases/download/v0/LSOA_Dec_2021_Boundaries_Generalised_Clipped_EW_BGC_2022_5605507071095448309.geojson\",\n                              bounds){\n  \ndir.create(file.path(tempdir(),\"lsoa\"))\nutils::download.file(url, destfile = file.path(tempdir(),\"lsoa\",\"LSOA_2021.geojson\"),\n                       mode = \"wb\")\n  res = sf::read_sf(file.path(tempdir(),\"lsoa\",\"LSOA_2021.geojson\"))\n  return(res)\n}\n\nlsoa &lt;- download_lsoa_2021()\n\nlsoa_fixed &lt;- lsoa |&gt; st_make_valid()\nlsoa_selected &lt;- lsoa_fixed[bounds,] |&gt; st_transform(4326)\nrm(lsoa,lsoa_fixed)\n\n\nlsoa_selected |&gt; \n  tm_shape()+\n  tm_polygons(col = \"blue\",\n              alpha = 0.3,\n              border.col = \"blue\")"
  },
  {
    "objectID": "C1_Census_data.html#data",
    "href": "C1_Census_data.html#data",
    "title": "Census data",
    "section": "Data",
    "text": "Data\n\nPopulation\n\ndir.create(file.path(tempdir(),\"census\"))\ndownload.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts001.zip\",\n              destfile = file.path(tempdir(),\"census\",\"pop_2021.zip\"),\n                       mode = \"wb\")\nunzip(file.path(tempdir(),\"census\",\"pop_2021.zip\"))\npop &lt;- read_csv(unz(file.path(tempdir(),\n                              \"census\",\n                              \"pop_2021.zip\"),\n                    \"census2021-ts001-lsoa.csv\"),\n                col_types = cols(\n  date = col_double(),\n  geography = col_character(),\n  `geography code` = col_character(),\n  `Residence type: Total; measures: Value` = col_double(),\n  `Residence type: Lives in a household; measures: Value` = col_double(),\n  `Residence type: Lives in a communal establishment; measures: Value` = col_double()\n)\n) |&gt; rename_with(.cols = starts_with(\"Residence\"),\n                 ~ gsub(\"Residence type: \",\"\",.x))\n\n\n\nExmployment Status\n\ndownload.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts066.zip\",\n              destfile = file.path(tempdir(),\"census\",\"employment_2021.zip\"),\n                       mode = \"wb\")\nunzip(file.path(tempdir(),\"census\",\"employment_2021.zip\"))\nemploy &lt;- read_csv(unz(file.path(tempdir(),\n                              \"census\",\n                              \"employment_2021.zip\"),\n                    \"census2021-ts066-lsoa.csv\")\n,\ncol_types = cols(\n  date = col_double(),\n  geography = col_character(),\n  `geography code` = col_character(),\n  `Economic activity status: Total: All usual residents aged 16 years and over` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students)` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment:Employee` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Employee: Part-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Employee: Full-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment:Self-employed with employees` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed with employees: Part-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed with employees: Full-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment:Self-employed without employees` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed without employees: Part-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed without employees: Full-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): Unemployed` = col_double(),\n  `Economic activity status: Economically active and a full-time student` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment:Employee` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Employee: Part-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Employee: Full-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment:Self-employed with employees` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed with employees: Part-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed with employees: Full-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment:Self-employed without employees` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed without employees: Part-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed without employees: Full-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: Unemployed` = col_double(),\n  `Economic activity status: Economically inactive` = col_double(),\n  `Economic activity status: Economically inactive: Retired` = col_double(),\n  `Economic activity status: Economically inactive: Student` = col_double(),\n  `Economic activity status: Economically inactive: Looking after home or family` = col_double(),\n  `Economic activity status: Economically inactive: Long-term sick or disabled` = col_double(),\n  `Economic activity status: Economically inactive: Other` = col_double()\n)\n) |&gt; rename_with(.cols = starts_with(\"Economic\"),\n                 ~ gsub(\"Economic activity status: \",\"\",.x))\n\n\n\nCar Availability\n\ndownload.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts045.zip\",\n              destfile = file.path(tempdir(),\"census\",\"car_avail_2021.zip\"),\n                       mode = \"wb\")\nunzip(file.path(tempdir(),\"census\",\"car_avail_2021.zip\"))\n\ncar_avail &lt;- read_csv(unz(file.path(tempdir(),\n                              \"census\",\n                              \"car_avail_2021.zip\"),\n                    \"census2021-ts045-lsoa.csv\"),\n                    col_types = cols(\n  date = col_double(),\n  geography = col_character(),\n  `geography code` = col_character(),\n  `Number of cars or vans: Total: All households` = col_double(),\n  `Number of cars or vans: No cars or vans in household` = col_double(),\n  `Number of cars or vans: 1 car or van in household` = col_double(),\n  `Number of cars or vans: 2 cars or vans in household` = col_double(),\n  `Number of cars or vans: 3 or more cars or vans in household` = col_double()\n)\n) |&gt; rename_with(.cols = starts_with(\"Number\"),\n                 ~ gsub(\"Number of cars or vans: \",\"\",.x))\n\n\ndownload.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts061.zip\",\n              destfile = file.path(tempdir(),\"census\",\"comm_mode_2021.zip\"),\n                       mode = \"wb\")\nunzip(file.path(tempdir(),\"census\",\"comm_mode_2021.zip\"))\n\ncomm_mode &lt;- read_csv(unz(file.path(tempdir(),\n                              \"census\",\n                              \"comm_mode_2021.zip\"),\n                    \"census2021-ts061-lsoa.csv\"),\n                    col_types = cols(\n  date = col_double(),\n  geography = col_character(),\n  `geography code` = col_character(),\n  `Method of travel to workplace: Total: All usual residents aged 16 years and over in employment the week before the census` = col_double(),\n  `Method of travel to workplace: Work mainly at or from home` = col_double(),\n  `Method of travel to workplace: Underground, metro, light rail, tram` = col_double(),\n  `Method of travel to workplace: Train` = col_double(),\n  `Method of travel to workplace: Bus, minibus or coach` = col_double(),\n  `Method of travel to workplace: Taxi` = col_double(),\n  `Method of travel to workplace: Motorcycle, scooter or moped` = col_double(),\n  `Method of travel to workplace: Driving a car or van` = col_double(),\n  `Method of travel to workplace: Passenger in a car or van` = col_double(),\n  `Method of travel to workplace: Bicycle` = col_double(),\n  `Method of travel to workplace: On foot` = col_double(),\n  `Method of travel to workplace: Other method of travel to work` = col_double()\n)\n) |&gt; rename_with(.cols = starts_with(\"Method\"),~ gsub(\"Method of travel to workplace: \",\"\",.x))\n\n\npbcc &lt;- download_pbcc() |&gt; semi_join(lsoa_selected,by = c(\"LSOA11NM\"=\"LSOA21NM\"))"
  }
]
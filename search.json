[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "Work in Progress\nThis web contains the code produced for a traffic estimation project for GEO5099.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "2A_Mod_fit.html",
    "href": "2A_Mod_fit.html",
    "title": "Fitting",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(brms)\n\nLoading required package: Rcpp\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\nAttaching package: 'brms'\n\nThe following object is masked from 'package:stats':\n\n    ar",
    "crumbs": [
      "Modelling",
      "Fitting"
    ]
  },
  {
    "objectID": "2A_Mod_fit.html#loading-data",
    "href": "2A_Mod_fit.html#loading-data",
    "title": "Fitting",
    "section": "Loading data",
    "text": "Loading data\n\nmodel_data_expanded &lt;- st_read(\"03_preprocessing_files/model_data.gpkg\")\n\nReading layer `model_data' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\model_data.gpkg' \n  using driver `GPKG'\nSimple feature collection with 15076 features and 21 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 504496.7 ymin: 426324.9 xmax: 514189.2 ymax: 434477.1\nProjected CRS: OSGB36 / British National Grid",
    "crumbs": [
      "Modelling",
      "Fitting"
    ]
  },
  {
    "objectID": "2A_Mod_fit.html#fitting-models",
    "href": "2A_Mod_fit.html#fitting-models",
    "title": "Fitting",
    "section": "Fitting models",
    "text": "Fitting models\n\nGLM model\n\nmodel_data_noNAs&lt;- model_data_expanded |&gt; \n  select(flow.2022,\n         flow.2023,\n         centrality,\n         car_avail_perc,\n         total_pop_c_1:wk_pop_c_5,\n         road_density) |&gt;\n  drop_na()\n\nA simple model with the centrality only for major roads\n\nGLM_models &lt;- lapply(\n  1:5,\n  function(i) {\n    myformula &lt;- as.formula(paste0(\n      \"flow.2023 ~ total_pop_c_\",\n      i,\n      \" + wk_pop_c_\",\n      i,\n      \" + road_density\"\n    ))\n    \n    m &lt;- glm(myformula,\n             data = model_data_noNAs,\n             family = poisson())\n  }\n)\n\n\n\nGWPR\n\nlibrary(GWmodel)\n\nLoading required package: robustbase\n\n\n\nAttaching package: 'robustbase'\n\n\nThe following object is masked from 'package:brms':\n\n    epilepsy\n\n\nLoading required package: sp\n\n\nWelcome to GWmodel version 2.3-2.\n\nmodel_data_noNAs_sp &lt;- as(model_data_noNAs |&gt; st_centroid(),\"Spatial\")\n\nWarning: st_centroid assumes attributes are constant over geometries\n\ndMat &lt;- model_data_noNAs |&gt; st_centroid() |&gt; st_coordinates() |&gt; gw.dist()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nGWPR_models &lt;- lapply(\n  1:5,\n  function(i){\n    myformula &lt;- as.formula(paste0(\n      \"flow.2023 ~ total_pop_c_\",\n      i,\n      \" + wk_pop_c_\",\n      i,\n      \" + road_density\"\n    ))\n    \n    # Determine the adaptive bandwidth\n    abw &lt;- bw.ggwr(\n      formula = myformula,\n      data = model_data_noNAs_sp,\n      family = \"poisson\",\n      approach = \"AICc\",\n      kernel = \"bisquare\",\n      adaptive = TRUE,\n      dMat = dMat\n    )\n    # Fit GWPR\n    gwpr.m &lt;- ggwr.basic(\n      myformula,\n      data = model_data_noNAs_sp,\n      family = \"poisson\",\n      bw = abw,\n      kernel = \"bisquare\",\n      adaptive = TRUE,\n      dMat = dMat\n    )\n    }\n  )\n\n Iteration    Log-Likelihood(With bandwidth:  89 )\n=========================\n       0      -1.268e+05 \n       1      -1.135e+05 \n       2      -1.13e+05 \n       3      -1.129e+05 \n       4      -1.129e+05 \n       5      -1.129e+05 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 225765 \n Iteration    Log-Likelihood(With bandwidth:  63 )\n=========================\n       0      -1.204e+05 \n       1      -1.077e+05 \n       2      -1.073e+05 \n       3      -1.071e+05 \n       4      -1.071e+05 \n       5      -1.071e+05 \nAdaptive bandwidth (number of nearest neighbours): 63 AICc value: 214243.6 \n Iteration    Log-Likelihood(With bandwidth:  45 )\n=========================\n       0      -1.164e+05 \n       1      -1.031e+05 \n       2      -1.028e+05 \n       3      -1.026e+05 \n       4      -1.025e+05 \n       5      -1.025e+05 \nAdaptive bandwidth (number of nearest neighbours): 45 AICc value: 205139.2 \n Iteration    Log-Likelihood(With bandwidth:  36 )\n=========================\n       0      -1.068e+05 \n       1      -9.409e+04 \n       2      -9.437e+04 \n       3      -9.409e+04 \n       4      -9.407e+04 \n       5      -9.407e+04 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 188203.1 \n Iteration    Log-Likelihood(With bandwidth:  28 )\n=========================\n       0      -9.358e+04 \n       1      -8.367e+04 \n       2      -8.379e+04 \n       3      -8.341e+04 \n       4      -8.338e+04 \n       5      -8.338e+04 \n       6      -8.338e+04 \nAdaptive bandwidth (number of nearest neighbours): 28 AICc value: 166845.5 \n Iteration    Log-Likelihood(With bandwidth:  25 )\n=========================\n       0      -9.064e+04 \n       1      -8.113e+04 \n       2      -8.11e+04 \n       3      -8.07e+04 \n       4      -8.066e+04 \n       5      -8.066e+04 \n       6      -8.066e+04 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 161420.2 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.652e+04 \n       1      -7.661e+04 \n       2      -7.634e+04 \n       3      -7.6e+04 \n       4      -7.597e+04 \n       5      -7.598e+04 \n       6      -7.598e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 152076 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.652e+04 \n       1      -7.661e+04 \n       2      -7.634e+04 \n       3      -7.6e+04 \n       4      -7.597e+04 \n       5      -7.598e+04 \n       6      -7.598e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 152076 \n Iteration    Log-Likelihood\n=========================\n       0      -8.652e+04 \n       1      -7.661e+04 \n       2      -7.634e+04 \n       3      -7.6e+04 \n       4      -7.597e+04 \n       5      -7.598e+04 \n       6      -7.598e+04 \n Iteration    Log-Likelihood(With bandwidth:  89 )\n=========================\n       0      -1.288e+05 \n       1      -1.139e+05 \n       2      -1.135e+05 \n       3      -1.133e+05 \n       4      -1.133e+05 \n       5      -1.133e+05 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 226701.4 \n Iteration    Log-Likelihood(With bandwidth:  63 )\n=========================\n       0      -1.208e+05 \n       1      -1.076e+05 \n       2      -1.072e+05 \n       3      -1.07e+05 \n       4      -1.07e+05 \n       5      -1.07e+05 \nAdaptive bandwidth (number of nearest neighbours): 63 AICc value: 213973.2 \n Iteration    Log-Likelihood(With bandwidth:  45 )\n=========================\n       0      -1.145e+05 \n       1      -1.005e+05 \n       2      -1.003e+05 \n       3      -1.001e+05 \n       4      -1.001e+05 \n       5      -1.001e+05 \nAdaptive bandwidth (number of nearest neighbours): 45 AICc value: 200158.5 \n Iteration    Log-Likelihood(With bandwidth:  36 )\n=========================\n       0      -1.071e+05 \n       1      -9.301e+04 \n       2      -9.286e+04 \n       3      -9.26e+04 \n       4      -9.259e+04 \n       5      -9.259e+04 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 185242.9 \n Iteration    Log-Likelihood(With bandwidth:  28 )\n=========================\n       0      -9.432e+04 \n       1      -8.288e+04 \n       2      -8.242e+04 \n       3      -8.215e+04 \n       4      -8.213e+04 \n       5      -8.214e+04 \nAdaptive bandwidth (number of nearest neighbours): 28 AICc value: 164359.7 \n Iteration    Log-Likelihood(With bandwidth:  25 )\n=========================\n       0      -9.069e+04 \n       1      -8.039e+04 \n       2      -7.976e+04 \n       3      -7.95e+04 \n       4      -7.948e+04 \n       5      -7.948e+04 \n       6      -7.948e+04 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 159058.9 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.677e+04 \n       1      -7.694e+04 \n       2      -7.6e+04 \n       3      -7.574e+04 \n       4      -7.572e+04 \n       5      -7.572e+04 \n       6      -7.572e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 151553.4 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.677e+04 \n       1      -7.694e+04 \n       2      -7.6e+04 \n       3      -7.574e+04 \n       4      -7.572e+04 \n       5      -7.572e+04 \n       6      -7.572e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 151553.4 \n Iteration    Log-Likelihood\n=========================\n       0      -8.677e+04 \n       1      -7.694e+04 \n       2      -7.6e+04 \n       3      -7.574e+04 \n       4      -7.572e+04 \n       5      -7.572e+04 \n       6      -7.572e+04 \n Iteration    Log-Likelihood(With bandwidth:  89 )\n=========================\n       0      -1.297e+05 \n       1      -1.157e+05 \n       2      -1.152e+05 \n       3      -1.151e+05 \n       4      -1.151e+05 \n       5      -1.151e+05 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 230133.5 \n Iteration    Log-Likelihood(With bandwidth:  63 )\n=========================\n       0      -1.2e+05 \n       1      -1.091e+05 \n       2      -1.086e+05 \n       3      -1.084e+05 \n       4      -1.084e+05 \n       5      -1.084e+05 \nAdaptive bandwidth (number of nearest neighbours): 63 AICc value: 216749.1 \n Iteration    Log-Likelihood(With bandwidth:  45 )\n=========================\n       0      -1.109e+05 \n       1      -1.01e+05 \n       2      -1.011e+05 \n       3      -1.008e+05 \n       4      -1.008e+05 \n       5      -1.008e+05 \nAdaptive bandwidth (number of nearest neighbours): 45 AICc value: 201642.4 \n Iteration    Log-Likelihood(With bandwidth:  36 )\n=========================\n       0      -1.01e+05 \n       1      -9.114e+04 \n       2      -9.164e+04 \n       3      -9.138e+04 \n       4      -9.136e+04 \n       5      -9.136e+04 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 182791.2 \n Iteration    Log-Likelihood(With bandwidth:  28 )\n=========================\n       0      -8.619e+04 \n       1      -8.032e+04 \n       2      -7.995e+04 \n       3      -7.974e+04 \n       4      -7.974e+04 \n       5      -7.975e+04 \n       6      -7.975e+04 \nAdaptive bandwidth (number of nearest neighbours): 28 AICc value: 159577.7 \n Iteration    Log-Likelihood(With bandwidth:  25 )\n=========================\n       0      -8.289e+04 \n       1      -7.749e+04 \n       2      -7.704e+04 \n       3      -7.682e+04 \n       4      -7.682e+04 \n       5      -7.682e+04 \n       6      -7.682e+04 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 153737.8 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -7.806e+04 \n       1      -7.185e+04 \n       2      -7.183e+04 \n       3      -7.154e+04 \n       4      -7.152e+04 \n       5      -7.153e+04 \n       6      -7.153e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 143164.2 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -7.806e+04 \n       1      -7.185e+04 \n       2      -7.183e+04 \n       3      -7.154e+04 \n       4      -7.152e+04 \n       5      -7.153e+04 \n       6      -7.153e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 143164.2 \n Iteration    Log-Likelihood\n=========================\n       0      -7.806e+04 \n       1      -7.185e+04 \n       2      -7.183e+04 \n       3      -7.154e+04 \n       4      -7.152e+04 \n       5      -7.153e+04 \n       6      -7.153e+04 \n Iteration    Log-Likelihood(With bandwidth:  89 )\n=========================\n       0      -1.241e+05 \n       1      -1.131e+05 \n       2      -1.127e+05 \n       3      -1.125e+05 \n       4      -1.125e+05 \n       5      -1.125e+05 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 224952.5 \n Iteration    Log-Likelihood(With bandwidth:  63 )\n=========================\n       0      -1.147e+05 \n       1      -1.071e+05 \n       2      -1.067e+05 \n       3      -1.064e+05 \n       4      -1.064e+05 \n       5      -1.064e+05 \nAdaptive bandwidth (number of nearest neighbours): 63 AICc value: 212827 \n Iteration    Log-Likelihood(With bandwidth:  45 )\n=========================\n       0      -1.098e+05 \n       1      -9.984e+04 \n       2      -9.978e+04 \n       3      -9.953e+04 \n       4      -9.951e+04 \n       5      -9.951e+04 \nAdaptive bandwidth (number of nearest neighbours): 45 AICc value: 199079.5 \n Iteration    Log-Likelihood(With bandwidth:  36 )\n=========================\n       0      -1.044e+05 \n       1      -9.159e+04 \n       2      -9.208e+04 \n       3      -9.181e+04 \n       4      -9.179e+04 \n       5      -9.179e+04 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 183644.2 \n Iteration    Log-Likelihood(With bandwidth:  28 )\n=========================\n       0      -9.535e+04 \n       1      -8.324e+04 \n       2      -8.346e+04 \n       3      -8.319e+04 \n       4      -8.316e+04 \n       5      -8.316e+04 \nAdaptive bandwidth (number of nearest neighbours): 28 AICc value: 166404.5 \n Iteration    Log-Likelihood(With bandwidth:  25 )\n=========================\n       0      -9.234e+04 \n       1      -8.037e+04 \n       2      -8.063e+04 \n       3      -8.033e+04 \n       4      -8.029e+04 \n       5      -8.029e+04 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 160676.7 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.643e+04 \n       1      -7.531e+04 \n       2      -7.55e+04 \n       3      -7.515e+04 \n       4      -7.509e+04 \n       5      -7.509e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 150300.7 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.643e+04 \n       1      -7.531e+04 \n       2      -7.55e+04 \n       3      -7.515e+04 \n       4      -7.509e+04 \n       5      -7.509e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 150300.7 \n Iteration    Log-Likelihood\n=========================\n       0      -8.643e+04 \n       1      -7.531e+04 \n       2      -7.55e+04 \n       3      -7.515e+04 \n       4      -7.509e+04 \n       5      -7.509e+04 \n Iteration    Log-Likelihood(With bandwidth:  89 )\n=========================\n       0      -1.262e+05 \n       1      -1.146e+05 \n       2      -1.142e+05 \n       3      -1.14e+05 \n       4      -1.14e+05 \n       5      -1.14e+05 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 228062.1 \n Iteration    Log-Likelihood(With bandwidth:  63 )\n=========================\n       0      -1.175e+05 \n       1      -1.08e+05 \n       2      -1.079e+05 \n       3      -1.077e+05 \n       4      -1.077e+05 \n       5      -1.077e+05 \nAdaptive bandwidth (number of nearest neighbours): 63 AICc value: 215374.1 \n Iteration    Log-Likelihood(With bandwidth:  45 )\n=========================\n       0      -1.11e+05 \n       1      -9.913e+04 \n       2      -9.981e+04 \n       3      -9.951e+04 \n       4      -9.949e+04 \n       5      -9.95e+04 \nAdaptive bandwidth (number of nearest neighbours): 45 AICc value: 199039.4 \n Iteration    Log-Likelihood(With bandwidth:  36 )\n=========================\n       0      -1.034e+05 \n       1      -9.035e+04 \n       2      -9.122e+04 \n       3      -9.092e+04 \n       4      -9.09e+04 \n       5      -9.09e+04 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 181865.8 \n Iteration    Log-Likelihood(With bandwidth:  28 )\n=========================\n       0      -9.338e+04 \n       1      -8.099e+04 \n       2      -8.154e+04 \n       3      -8.128e+04 \n       4      -8.124e+04 \n       5      -8.124e+04 \nAdaptive bandwidth (number of nearest neighbours): 28 AICc value: 162567.3 \n Iteration    Log-Likelihood(With bandwidth:  25 )\n=========================\n       0      -9.128e+04 \n       1      -7.836e+04 \n       2      -7.891e+04 \n       3      -7.863e+04 \n       4      -7.858e+04 \n       5      -7.858e+04 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 157262.6 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.628e+04 \n       1      -7.377e+04 \n       2      -7.425e+04 \n       3      -7.397e+04 \n       4      -7.391e+04 \n       5      -7.391e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 147931.5 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -8.628e+04 \n       1      -7.377e+04 \n       2      -7.425e+04 \n       3      -7.397e+04 \n       4      -7.391e+04 \n       5      -7.391e+04 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 147931.5 \n Iteration    Log-Likelihood\n=========================\n       0      -8.628e+04 \n       1      -7.377e+04 \n       2      -7.425e+04 \n       3      -7.397e+04 \n       4      -7.391e+04 \n       5      -7.391e+04 \n\n\n\n\nBayesian Negative Binomial regression\n\nlibrary(brms)\nBayes_NBin_models &lt;- lapply(\n  1:5,\n  function(i) {\n    myformula &lt;- as.formula(paste0(\n      \"flow.2023 ~ total_pop_c_\",\n      i,\n      \" + wk_pop_c_\",\n      i,\n      \" + road_density\"\n    ))\n    \n    m &lt;- brm(myformula,\n             data = model_data_noNAs,\n             family = negbinomial())\n  }\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.228 seconds (Warm-up)\nChain 1:                0.234 seconds (Sampling)\nChain 1:                0.462 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.72 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.204 seconds (Warm-up)\nChain 2:                0.186 seconds (Sampling)\nChain 2:                0.39 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.206 seconds (Warm-up)\nChain 3:                0.162 seconds (Sampling)\nChain 3:                0.368 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.197 seconds (Warm-up)\nChain 4:                0.2 seconds (Sampling)\nChain 4:                0.397 seconds (Total)\nChain 4: \n\n\nCompiling Stan program...\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.46 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.209 seconds (Warm-up)\nChain 1:                0.187 seconds (Sampling)\nChain 1:                0.396 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.8e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.228 seconds (Warm-up)\nChain 2:                0.206 seconds (Sampling)\nChain 2:                0.434 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.46 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.203 seconds (Warm-up)\nChain 3:                0.19 seconds (Sampling)\nChain 3:                0.393 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.197 seconds (Warm-up)\nChain 4:                0.205 seconds (Sampling)\nChain 4:                0.402 seconds (Total)\nChain 4: \n\n\nCompiling Stan program...\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000102 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.02 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.265 seconds (Warm-up)\nChain 1:                0.19 seconds (Sampling)\nChain 1:                0.455 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.253 seconds (Warm-up)\nChain 2:                0.202 seconds (Sampling)\nChain 2:                0.455 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000117 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.17 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.248 seconds (Warm-up)\nChain 3:                0.22 seconds (Sampling)\nChain 3:                0.468 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000114 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.14 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.259 seconds (Warm-up)\nChain 4:                0.2 seconds (Sampling)\nChain 4:                0.459 seconds (Total)\nChain 4: \n\n\nCompiling Stan program...\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.225 seconds (Warm-up)\nChain 1:                0.232 seconds (Sampling)\nChain 1:                0.457 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.262 seconds (Warm-up)\nChain 2:                0.194 seconds (Sampling)\nChain 2:                0.456 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.245 seconds (Warm-up)\nChain 3:                0.205 seconds (Sampling)\nChain 3:                0.45 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.239 seconds (Warm-up)\nChain 4:                0.214 seconds (Sampling)\nChain 4:                0.453 seconds (Total)\nChain 4: \n\n\nCompiling Stan program...\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.49 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.263 seconds (Warm-up)\nChain 1:                0.199 seconds (Sampling)\nChain 1:                0.462 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.276 seconds (Warm-up)\nChain 2:                0.198 seconds (Sampling)\nChain 2:                0.474 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.247 seconds (Warm-up)\nChain 3:                0.243 seconds (Sampling)\nChain 3:                0.49 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 8.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.272 seconds (Warm-up)\nChain 4:                0.195 seconds (Sampling)\nChain 4:                0.467 seconds (Total)\nChain 4:",
    "crumbs": [
      "Modelling",
      "Fitting"
    ]
  },
  {
    "objectID": "2A_Mod_fit.html#null-models",
    "href": "2A_Mod_fit.html#null-models",
    "title": "Fitting",
    "section": "Null models",
    "text": "Null models\n\nGLM\n\nnull.glm &lt;- glm(flow.2023 ~ 1,\n              data = model_data_noNAs,\n              family = poisson())\n\n\n\nGWPR\n\nabw &lt;- bw.ggwr(\n  formula = flow.2023 ~ 1,\n  data = model_data_noNAs_sp,\n  family = \"poisson\",\n  approach = \"AICc\",\n  kernel = \"bisquare\",\n  adaptive = TRUE,\n  dMat = dMat\n)\n\n Iteration    Log-Likelihood(With bandwidth:  89 )\n=========================\n       0      -1.413e+05 \n       1      -1.333e+05 \n       2      -1.334e+05 \n       3      -1.333e+05 \n       4      -1.333e+05 \n       5      -1.333e+05 \nAdaptive bandwidth (number of nearest neighbours): 89 AICc value: 266536 \n Iteration    Log-Likelihood(With bandwidth:  63 )\n=========================\n       0      -1.39e+05 \n       1      -1.296e+05 \n       2      -1.299e+05 \n       3      -1.297e+05 \n       4      -1.297e+05 \n       5      -1.297e+05 \nAdaptive bandwidth (number of nearest neighbours): 63 AICc value: 259472.8 \n Iteration    Log-Likelihood(With bandwidth:  45 )\n=========================\n       0      -1.367e+05 \n       1      -1.267e+05 \n       2      -1.267e+05 \n       3      -1.265e+05 \n       4      -1.265e+05 \n       5      -1.265e+05 \nAdaptive bandwidth (number of nearest neighbours): 45 AICc value: 253087.6 \n Iteration    Log-Likelihood(With bandwidth:  36 )\n=========================\n       0      -1.324e+05 \n       1      -1.217e+05 \n       2      -1.219e+05 \n       3      -1.217e+05 \n       4      -1.217e+05 \n       5      -1.217e+05 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 243486.9 \n Iteration    Log-Likelihood(With bandwidth:  28 )\n=========================\n       0      -1.28e+05 \n       1      -1.136e+05 \n       2      -1.145e+05 \n       3      -1.143e+05 \n       4      -1.143e+05 \n       5      -1.143e+05 \nAdaptive bandwidth (number of nearest neighbours): 28 AICc value: 228677.5 \n Iteration    Log-Likelihood(With bandwidth:  25 )\n=========================\n       0      -1.268e+05 \n       1      -1.113e+05 \n       2      -1.123e+05 \n       3      -1.122e+05 \n       4      -1.122e+05 \n       5      -1.122e+05 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 224351.3 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -1.247e+05 \n       1      -1.08e+05 \n       2      -1.092e+05 \n       3      -1.091e+05 \n       4      -1.091e+05 \n       5      -1.091e+05 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 218188.9 \n Iteration    Log-Likelihood(With bandwidth:  21 )\n=========================\n       0      -1.247e+05 \n       1      -1.08e+05 \n       2      -1.092e+05 \n       3      -1.091e+05 \n       4      -1.091e+05 \n       5      -1.091e+05 \nAdaptive bandwidth (number of nearest neighbours): 21 AICc value: 218188.9 \n\n# Fit GWPR\nnull.gwpr &lt;- ggwr.basic(\n  flow.2023 ~ 1,\n  data = model_data_noNAs_sp,\n  family = \"poisson\",\n  bw = abw,\n  kernel = \"bisquare\",\n  adaptive = TRUE,\n  dMat = dMat\n)\n\n Iteration    Log-Likelihood\n=========================\n       0      -1.247e+05 \n       1      -1.08e+05 \n       2      -1.092e+05 \n       3      -1.091e+05 \n       4      -1.091e+05 \n       5      -1.091e+05 \n\n\n\n\nBayesian\n\nnull.bnb &lt;- brm(flow.2023 ~ 1,\n                data = model_data_noNAs,\n                family = negbinomial())\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.91 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.229 seconds (Warm-up)\nChain 1:                0.159 seconds (Sampling)\nChain 1:                0.388 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.161 seconds (Warm-up)\nChain 2:                0.15 seconds (Sampling)\nChain 2:                0.311 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.88 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.226 seconds (Warm-up)\nChain 3:                0.142 seconds (Sampling)\nChain 3:                0.368 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.165 seconds (Warm-up)\nChain 4:                0.126 seconds (Sampling)\nChain 4:                0.291 seconds (Total)\nChain 4:",
    "crumbs": [
      "Modelling",
      "Fitting"
    ]
  },
  {
    "objectID": "2A_Mod_fit.html#saving-the-data",
    "href": "2A_Mod_fit.html#saving-the-data",
    "title": "Fitting",
    "section": "Saving the data",
    "text": "Saving the data\n\nsave(GLM_models,\n     GWPR_models,\n     Bayes_NBin_models,\n     null.glm,\n     null.gwpr,\n     null.bnb,\n     file = \"03_preprocessing_files/fitted_models.RData\")",
    "crumbs": [
      "Modelling",
      "Fitting"
    ]
  },
  {
    "objectID": "1C_Road_network.html",
    "href": "1C_Road_network.html",
    "title": "Road Network",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dodgr)\nlibrary(osmextract)\nlibrary(sf)\nlibrary(tmap)\nlibrary(stplanr)\nThe following code uses some functions compiled in the MinorRoadTraffic repository prepared by Malcolm Morgan and follows the process described in this vignette of the same package.\n# remotes::install_github(\"ITSLeeds/MinorRoadTraffic\") # if not installed\nlibrary(MinorRoadTraffic)\n\n\nAttaching package: 'MinorRoadTraffic'\n\n\nThe following object is masked from 'package:stplanr':\n\n    line_segment",
    "crumbs": [
      "Preprocessing",
      "Road Network"
    ]
  },
  {
    "objectID": "1C_Road_network.html#downloading-data",
    "href": "1C_Road_network.html#downloading-data",
    "title": "Road Network",
    "section": "Downloading data",
    "text": "Downloading data\nFor this analysis, OpenStreetMap will be used for the road network. Network data will be downloaded in silicate format\nThe study area bounds will be used to clip the road network, these are read from the files produced before\n\nsf_counts &lt;- st_read(\"03_preprocessing_files/grouped_counts.geojson\")\n\nReading layer `grouped_counts' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\grouped_counts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 185 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 505930.6 ymin: 428325.3 xmax: 513352.9 ymax: 433636.7\nProjected CRS: OSGB36 / British National Grid\n\nbounds &lt;- st_read(dsn = \"03_preprocessing_files/bounds.geoJSON\")\n\nReading layer `bounds' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\bounds.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 501634.1 ymin: 423660 xmax: 517322.2 ymax: 437607\nProjected CRS: OSGB36 / British National Grid\n\n\nA bbox is generated from the bounds to be used to extract the OSM data with dodgr_streetnet_sc\n\nbbox_hull &lt;- bounds |&gt; st_transform(4326) |&gt; st_bbox() |&gt; as.numeric()\n\n\nif(!file.exists(\"03_preprocessing_files/osm_sc.rds\")) {\n  # This is required when firewall restrictions apply\n  assign(\"has_internet_via_proxy\",\n         TRUE,\n         environment(curl::has_internet))\n  osm_sc &lt;- dodgr_streetnet_sc(bbox = bbox_hull)\n  write_rds(osm_sc, \"03_preprocessing_files/osm_sc.rds\")\n} else {\n  osm_sc &lt;- read_rds(\"03_preprocessing_files/osm_sc.rds\")\n}\n\nAn alternative source can be the OS Open Roads.",
    "crumbs": [
      "Preprocessing",
      "Road Network"
    ]
  },
  {
    "objectID": "1C_Road_network.html#pre-processing-osm",
    "href": "1C_Road_network.html#pre-processing-osm",
    "title": "Road Network",
    "section": "Pre-Processing OSM",
    "text": "Pre-Processing OSM",
    "crumbs": [
      "Preprocessing",
      "Road Network"
    ]
  },
  {
    "objectID": "1C_Road_network.html#join-with-other-datasets",
    "href": "1C_Road_network.html#join-with-other-datasets",
    "title": "Road Network",
    "section": "Join with other datasets",
    "text": "Join with other datasets\n\nTraffic counts\n\naadf_data &lt;- read_csv(\n  \"03_preprocessing_files/aadf_data.csv\",\n  col_types = cols(\n    group_id = col_double(),\n    subgroup_id = col_double(),\n    flow.2022 = col_double(),\n    flow.2023 = col_double()\n    )\n  )\n\nJoining the counts and the spatial data\n\nsf_aadf &lt;- sf_counts |&gt; \n  left_join(aadf_data,by = join_by(group_id, subgroup_id))\n\nIt is assumed that the locations of the loops are correct and the uncertainty of the coordinates is very small.\n\nThe assign_aadt_major of the MinorRoadsTraffic package uses voronoi diagrams to assign the traffic flows to the major roads of the network. The assignment will be different for this analysis for the following reasons:\n\nThe hull dataset has unidirectional traffic counts, as opposed to the bi-directional counts of the dataset by DfT .\nTraffic counts represent the flow in a specific section of the road links, other sections before and after adjacent junctions might be different.\n\nA graph is created using the dodgr package\n\nmajor_ref = c(\"motorway\",\n              \"motorway_link\",\n              \"primary\",\n              \"primary_link\",\n              \"trunk\",\n              \"trunk_link\",\n              \"secondary\",\n              \"secondary_link\",\n              \"tertiary\",\n              \"tertiary_link\")\n\nroad_types = c(\"motorway\", \"motorway_link\", \"trunk\", \"trunk_link\", \"primary\",\n               \"primary_link\", \"secondary\", \"secondary_link\",\n               \"tertiary\", \"tertiary_link\",\n               \"unclassified\", \"living_street\", \"residential\")\n\ngraph &lt;- dodgr::weight_streetnet(osm_sc,\n                                 keep_cols = c(\"name\",\"ref\",\"highway\",\"junction\",\"maxspeed\",\"oneway\"),\n                                 wt_profile = \"motorcar\"\n                                 )\n\nLoading required namespace: geodist\n\n\nAs the network has been clipped, it is possible to find some disconnected parts from the main network, these smaller component will be discarded; additionally, duplicated edges are discarded.\n\nmain_component &lt;- tibble(component = graph$component) |&gt; \n  count(component) |&gt;\n  filter(n == max(n)) |&gt;\n  pull(component)\n\n# Discarding unconnected roads and not public roads\ngraph_c1 &lt;- graph[graph$component %in% main_component & graph$highway %in% road_types,] |&gt;\n  dodgr_deduplicate_graph()\n\nBetweenness centrality is calculated using the contracted graph and dodgr\n\ngraph_clean &lt;- graph_c1 |&gt;  \n  dodgr_centrality(contract = T)\n\nAn sf object is produced from the clean network\n\ngraph_sf &lt;- dodgr::dodgr_to_sf(graph_clean) |&gt;\n  st_transform(27700)\n\nA quick visualisation\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(bounds)+\n  tm_polygons(alpha = 0.3)+\n  tm_shape(graph_sf)+\n  tm_lines(col = \"orange\",alpha = 0.4)+\n  tm_shape(sf_aadf)+\n  tm_dots(col = \"blue\")\n\n\n\n\n\n\n\n\nLinks within 20 metres from the count are considered and then filtered using the stated direction; then the nearest feature/link with the correct bearing is selected. If two or more counts are assigned to the same road link/graph, the mean flow is calculated.\n\ntraffic_buffer &lt;- sf_aadf |&gt;\n  st_buffer(20)\n\nroad_intersects &lt;- st_intersects(traffic_buffer,graph_sf)\n\ndirs_tbl &lt;-\n  tibble(\n    dir_str = c(\n      \"southbound\",\n      \"westbound\",\n      \"northbound\",\n      \"eastbound\",\n      \"northeastbound\",\n      \"northwestbound\",\n      \"southeastbound\",\n      \"southwestbound\"\n    ),\n    t_bearing = c(180, -90, 0, 90,\n                  45, -45, -135, 135)\n  )\n\n#Bearing tolerance\nb_tolerance &lt;- 90\n\n\nsf_aadf$edge_id &lt;- vapply(seq_len(nrow(traffic_buffer)),\n       function(t_count){\n  str_dir_bearing &lt;- dirs_tbl$t_bearing[traffic_buffer$dir_str[t_count]==dirs_tbl$dir_str]\n  \n  sel_road_links = graph_sf[road_intersects[[t_count]],]\n  \n  if(nrow(sel_road_links)&gt;0) {\n    sel_road_links$bearing = stplanr::line_bearing(l = sel_road_links |&gt;\n                                                     st_transform(crs = 4326))\n    \n    sel_road_links$bearing_check = (str_dir_bearing - abs(sel_road_links$bearing))  &lt; b_tolerance\n    \n    sel_road_links = sel_road_links[sel_road_links$bearing_check, ]\n    \n    t_edge = sel_road_links[st_nearest_feature(sf_aadf[t_count, ], sel_road_links), ] |&gt;\n      st_drop_geometry()\n    \n    return(t_edge$edge_id)\n  } else{\n    return(NA_character_)\n  }\n  \n  },\n  character(1))\n\nedge_flow &lt;- sf_aadf |&gt;\n  st_drop_geometry() |&gt;\n  drop_na(edge_id) |&gt;\n  summarise(across(starts_with(\"flow\"),mean),.by = edge_id)\n\nA final version of the sf object of the network with the known flows is produced\n\ngraph_contr = dodgr::dodgr_contract_graph(graph_clean)\n\ngraph_sf_flows &lt;- graph_sf |&gt;\n  left_join(edge_flow,by=\"edge_id\")\n\ngraph_sf_flows$road_type &lt;-\n  dplyr::if_else(graph_sf_flows$highway %in% major_ref,\n                 \"major\",\n                 \"minor\")\n\n\ntm_shape(graph_sf_flows)+\n  tm_lines(col = \"road_type\",lwd = 1,palette = \"-Greens\")\n\n\n\n\n\n\n\n\nThe following map shows the road network and the flows that have been assigned\n\ntmap_mode(\"plot\")\ntm_shape(sf_aadf)+\n  tm_dots(\"blue\",alpha=0.5)+\n  tm_shape(graph_sf_flows |&gt; filter(is.na(flow.2023)))+\n  tm_lines(\"grey\",lwd = 1)+\n  tm_shape(graph_sf_flows |&gt; drop_na(flow.2023))+\n  tm_lines(\"flow.2023\",lwd = 2.5)\n\n\n\n\n\n\n\n\n\nJunctions\nJunctions are extracted from the contracted graph to avoid redundant nodes\n\njunctions &lt;- dodgr::dodgr_vertices(graph_contr) |&gt;\n  st_as_sf(coords = c(\"x\",\"y\"),\n           crs = 4326)\n\nJunctions will be classified as minor, major and minor-major depending on the adjacent links.\n\njunction_class_to &lt;- graph_sf_flows |&gt;\n  st_drop_geometry() |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- graph_sf_flows |&gt;\n  st_drop_geometry() |&gt;\n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\nrm(junction_class_from,junction_class_to)\n\nMean incoming approach flows are calculated for all junctions, i.e. if a junction has 4 incoming roads/edges, an average of the 4 is calculated. Outgoing flows are not considered to avoid double-counting since the in and out flows should be the same.\n\njunctions_in_flows &lt;- graph_sf_flows |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(starts_with(\"flow\"),\\(x) mean(x,na.rm = T)),.by = c(to_id))|&gt; \n  rename(id = to_id)\n\n\njunctions_flows &lt;- junctions |&gt; \n  left_join(junctions_in_flows,by = \"id\") |&gt; \n  left_join(junctions_classed,by = \"id\")\nrm(junctions_classed,junctions_in_flows,junctions)\n\nThe following map shows the average flow in Minor-Major junctions.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(graph_sf_flows)+\n  tm_lines(\"#87CEFA\",lwd = 1)+\n  tm_shape(junctions_flows |&gt; filter(jct_type == \"minmaj\"))+\n  tm_dots(col = \"flow.2023\",size = 0.2, title = \"AADF 2023\")\n\n\n\n\n\n\n\n\n\n\n\nLSOA data\n\nlsoa_data &lt;- st_read(\"03_preprocessing_files/LSOA_data.gpkg\")\n\nReading layer `LSOA_data' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\LSOA_data.gpkg' \n  using driver `GPKG'\nSimple feature collection with 259 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 488932.5 ymin: 411729 xmax: 527121.4 ymax: 446712.1\nProjected CRS: OSGB36 / British National Grid\n\n\nRoad links are spatially joined to the LSOA based on the largest overlap.\n\nCatchment of individual links\nIt will be assumed that the length of each road will be proportional to the population served in each LSOA, i.e. if there are two roads in an LSOA, one with 750 m and the other with 250 m, their distribution will be assumed as 75% and 25% respectively.\n\nedge_lsoa &lt;- graph_sf_flows |&gt;\n  select(edge_id,d,road_type) |&gt;\n  st_join(lsoa_data |&gt; select(LSOA21CD),\n          largest = T) |&gt;\n  st_drop_geometry() |&gt; \n  mutate(portion_lsoa = d/sum(d,na.rm = T),.by = c(LSOA21CD,road_type)) |&gt; \n  mutate(portion_lsoa = if_else(road_type==\"major\" | is.na(LSOA21CD),0,portion_lsoa)) |&gt; \n  select(-d,-road_type)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries",
    "crumbs": [
      "Preprocessing",
      "Road Network"
    ]
  },
  {
    "objectID": "1C_Road_network.html#network-metrics",
    "href": "1C_Road_network.html#network-metrics",
    "title": "Road Network",
    "section": "Network metrics",
    "text": "Network metrics\n\nRoad density\nThis is an improved version of the road_density function in the MinorRoadTraffic package\n\nzone_inter &lt;- sf::st_intersects(lsoa_data, graph_sf_flows)\n\nlsoa_data$road_km &lt;- pbapply::pbvapply(seq_along(zone_inter),\n                             function(zone) {\n                               t_zone &lt;- lsoa_data[zone,]\n                               line &lt;- graph_sf_flows[zone_inter[[zone]],]\n                               \n                               if (nrow(line) == 0) {\n                                 return(0)\n                               }\n                               \n                               line &lt;- sf::st_intersection(t_zone, line)\n                               return(sum(as.numeric(sf::st_length(line))) / 1000)\n                             },\n                             FUN.VALUE = numeric(1))\n\nlsoa_data_density &lt;-\n  lsoa_data |&gt; mutate(area_km2 = as.numeric(st_area(geom)) / 1e6,\n                      road_density = road_km / area_km2)\n\nA map of the road density in \\(km/km^2\\):\n\ntm_shape(lsoa_data_density)+\n  tm_polygons(col = \"road_density\",\n              title = \"road km/sq.km\",\n              palette = \"Greens\",\n              border.col = \"grey\")\n\n\n\n\n\n\n\n\n\n\nJunction density\nSimilarly, the junction is calculated with the following code:\n\nlsoa_junctions &lt;- sf::st_intersects(lsoa_data,\n                                    junctions_flows |&gt; st_transform(27700))\n\nlsoa_data_density$jct_counts &lt;- pbapply::pbvapply(lsoa_junctions,length,integer(1))\n\nlsoa_data_density2 &lt;-\n  lsoa_data_density |&gt; mutate(area_km2 = as.numeric(st_area(geom)) / 1e6,\n                      jct_density = jct_counts / area_km2)\n\nA map with the junction density\n\ntm_shape(lsoa_data_density2)+\n  tm_polygons(col = \"jct_density\",\n              title = \"Junctions/sq.km\",\n              palette = c(\"white\",\"blue\",\"darkblue\"),\n              border.col = \"grey\")\n\n\n\n\n\n\n\n\n\n\nCentrality\nThe values of centrality calculated before are scaled dividing by the maximum value\n\nnpoints &lt;- dodgr_vertices(graph_c1) |&gt; count(component) |&gt; pull(n)\n\ngraph_sf_centrality &lt;- graph_sf_flows |&gt;\n  mutate(std.centrality = centrality/((npoints-1)*(npoints-2)))\n\nA map to explore the centrality results\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(graph_sf_centrality)+\n  tm_lines(col = \"std.centrality\",palette = \"viridis\",lwd = 1,style = \"fisher\")\n\n\n\n\n\n\n\n\n\n\nCatchment for major roads\nPoints withing iso-chrone bands are extracted using the graph\n\nnodes_list &lt;- junctions_flows |&gt; \n  filter(jct_type!=\"minor\") |&gt; pull(id) |&gt; unique()\n\n# Distance bands\ntlim &lt;- c (1,2,3,4,5)*60\n\nd &lt;- dodgr_isoverts(graph_contr, from = nodes_list,tlim =  tlim) |&gt; \n    mutate(tlim = tlim / 60)\n\nAn example of the results for one of the nodes\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nnodes_sample &lt;-c(\"858681448\", d |&gt;\nfilter(from==\"858681448\") |&gt; \n  pull(id))\n\ntm_shape(graph_sf |&gt; \n  filter(from_id %in% nodes_sample&to_id %in% nodes_sample))+\n  tm_lines(\"skyblue\")+\n  tm_shape(d |&gt;\n  st_as_sf(coords = c(\"x\",\"y\"),crs = 4326) |&gt;\n  filter(from == \"858681448\"))+\n  tm_dots(\"tlim\",size = 0.1,breaks = c (0,1,2,3,4,5,6),title = \"Catchment (min)\")+\n  tm_shape(junctions_flows |&gt; \n             filter(id == \"858681448\"))+\n  tm_dots(\"red\",size = 1)\n\n\n\n\n\n\n\n\n\n\nDistance from minor-major junctions\nFirst the mid-point of all lines are extracted\n\nmidpoints &lt;- dodgr::dodgr_to_sf(graph_clean) |&gt;\n  st_drop_geometry() |&gt;\n  select(edge_id) |&gt; \n  bind_cols(\n    dodgr::dodgr_to_sf(graph_clean) |&gt; \n  st_coordinates() |&gt; \n  data.frame() |&gt; \n  tibble() |&gt; \n  filter(row_number()==ceiling(n()/2),.by =L1) |&gt;\n  select(-L1)) |&gt; \n  # Extracting ids from uncontracted graph\n  left_join(graph_clean |&gt; dodgr_vertices(),\n            by = c(\"X\"=\"x\",\"Y\"=\"y\"))\n\nJunctions with known flows are sub-set and then distances from all mid-points to these junctions are calculated.\n\nid_jct_flows &lt;- junctions_flows |&gt; \n  filter(!is.na(flow.2023))\n\ntimes_matrix_flows &lt;- dodgr_times(graph_clean,\n                                  from = id_jct_flows$id,\n                                  to = midpoints$id,\n                                  shortest = F)\n\nChecking the number of unconnected links. Zero are expected as we are analysing the main component of the graph\n\nlength(colSums(times_matrix_flows)[is.na(colSums(times_matrix_flows,na.rm = T))])\n\n[1] 0\n\n\nUsing the times matrix, we extract the id of the nearest junction with known flows\n\nfastest_all &lt;- tibble(\n  id.jct = \n    apply(times_matrix_flows, 2,\n          \\(x) {id_jct_flows$id[match(min(x,na.rm = TRUE), x)]}\n    ),\n  dist.jct =\n    apply(times_matrix_flows, 2,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\nJoining the id of the nearest known count and the distance.\n\nmidpoints_jct &lt;- midpoints |&gt; select(edge_id) |&gt; bind_cols(fastest_all)\n\ngraph_sf_centrality |&gt;\n  left_join(midpoints_jct,\n            by = join_by(edge_id)) |&gt; \n  tm_shape()+\n  tm_lines(\"dist.jct\")",
    "crumbs": [
      "Preprocessing",
      "Road Network"
    ]
  },
  {
    "objectID": "1C_Road_network.html#consolidating-all-data",
    "href": "1C_Road_network.html#consolidating-all-data",
    "title": "Road Network",
    "section": "Consolidating all data",
    "text": "Consolidating all data\n\nsf_net_full_data &lt;- graph_sf_centrality |&gt;\n  left_join(midpoints_jct,\n            by = join_by(edge_id)) |&gt; \n  left_join(junctions_flows |&gt;\n              st_drop_geometry() |&gt;\n              select(id,flow.2022,flow.2023),\n            by = c(\"id.jct\"=\"id\"),\n            suffix = c(\"\",\".jct\")) |&gt; \n  left_join(edge_lsoa,by = join_by(edge_id))",
    "crumbs": [
      "Preprocessing",
      "Road Network"
    ]
  },
  {
    "objectID": "1C_Road_network.html#saving-data",
    "href": "1C_Road_network.html#saving-data",
    "title": "Road Network",
    "section": "Saving data",
    "text": "Saving data\n\nst_write(sf_net_full_data, \"03_preprocessing_files/network_data.gpkg\",\n         layer = \"network_data\",\n         delete_layer = T,\n         append = T)\n\nDeleting layer `network_data' using driver `GPKG'\nUpdating layer `network_data' to data source `03_preprocessing_files/network_data.gpkg' using driver `GPKG'\nWriting 46294 features with 32 fields and geometry type Line String.\n\nst_write(lsoa_data_density2,\n         \"03_preprocessing_files/network_data.gpkg\",\n         layer = \"LSOA\",\n         delete_layer = T,\n         append = T)\n\nDeleting layer `LSOA' using driver `GPKG'\nUpdating layer `LSOA' to data source `03_preprocessing_files/network_data.gpkg' using driver `GPKG'\nWriting 259 features with 15 fields and geometry type Multi Polygon.\n\nst_write(junctions_flows,\n         \"03_preprocessing_files/network_data.gpkg\",\n         layer = \"junctions\",\n         delete_layer = T,\n         append = T)\n\nDeleting layer `junctions' using driver `GPKG'\nUpdating layer `junctions' to data source `03_preprocessing_files/network_data.gpkg' using driver `GPKG'\nWriting 17874 features with 6 fields and geometry type Point.\n\nwrite_csv(d,file = \"03_preprocessing_files/junctions_catchment.csv\")",
    "crumbs": [
      "Preprocessing",
      "Road Network"
    ]
  },
  {
    "objectID": "1A_Count_data.html",
    "href": "1A_Count_data.html",
    "title": "Count Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(rjson)\nlibrary(httr)\nlibrary(sf)\nlibrary(tmap)\n\n\nSensor summary\nThe following code loads the file with the summary of scoot loop sensors in Hull\n\nloop_Locations  &lt;- GET(\"https://opendata.hullcc.gov.uk/dataset/30fd3969-556d-4eae-ae4c-f3f9d2cfa9e3/resource/90e1cce0-295e-4fa7-aa21-ebc4f3e8e8d4/download/scoot_loop_resources_full.json\")\n\nmy_response &lt;- rjson::fromJSON(content(loop_Locations,'text',encoding = \"UTF-8\"))\n\nmy_data &lt;- data.frame(do.call(rbind,\n                             lapply(my_response,\n                                    rbind))) |&gt;\n  unnest(cols = everything()) |&gt;\n  filter(longitude != 0)\n\nA spatial object is created using the coordinates of the sites\n\nsf_cameras &lt;- my_data |&gt; \n  st_as_sf(\n    coords = c(\"longitude\",\"latitude\"),\n    crs = 4326) |&gt;\n  st_transform(crs = 27700)\n\n\ntm_basemap() +\n        tm_shape(sf_cameras) +\n        tm_dots()\n\n\n\n\n\n\n\n\nThere are some issues with the coordinates of some sites. The following code fixes the problem\n\nmy_data[my_data$latitude&lt;0,c(\"longitude\",\"latitude\")] &lt;- rev(my_data[my_data$latitude&lt;0,c(\"longitude\",\"latitude\")])\n\nThe description column contain useful information on the location of each sensor, this can be used to identify sensors corresponding to different lanes in the same road, for example.\n\nmy_data_expanded &lt;- my_data |&gt;\n  mutate(description = str_replace(description,\"HE SITE - \",\"HE SITE _ \")) |&gt; \n  separate_wider_delim(description,delim = \" - \",names = c(\"desc\",\"direction\",\"url\",\"coord\")) |&gt;\n  select(-coord)\n\n\nsf_sensors_raw &lt;- my_data_expanded |&gt; \n  st_as_sf(\n    coords = c(\"longitude\",\"latitude\"),\n    crs = 4326) |&gt;\n  st_transform(crs = 27700)\n\ntm_basemap(\"OpenStreetMap\") +\n        tm_shape(sf_sensors_raw) +\n        tm_dots()\n\n\n\n\n\n\n\n\n\nGrouping sensors\n\nsensors_buffer = sf_sensors_raw |&gt;\n  st_buffer(dist = 20)\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_basemap(\"OpenStreetMap\") +\n  tm_shape(sensors_buffer |&gt; \n             filter(name %in% c(\"N44131F\",\"N44131X\",\"N44131H\"))) +\n        tm_polygons(alpha = 0.4)+\n        tm_shape(sf_sensors_raw) +\n        tm_dots()\n\n\n\n\n\n\nbuffer_groups  &lt;- st_cast(st_union(sensors_buffer),\"POLYGON\")\nsensors_overlap &lt;- st_intersects(buffer_groups,sf_sensors_raw)\nsensors_overlap\n\nSparse geometry binary predicate list of length 189, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 285\n 2: 287\n 3: 286\n 4: 12\n 5: 5\n 6: 40\n 7: 15, 17\n 8: 13, 16\n 9: 14\n 10: 9\n\n\n\n\nFlow Direction refining\nThe description of the count sites refers to the junction where the induction loops are installed, and not necessarily correspond to actual road where they are installed.\nThe sites shown in the map below, for example, are all described as “TSI047 COUNTY ROAD / BRICKNALL AVE”, but they correspond to the four arms of the junction with sensors on Fairfax Ave, Bricknell Ave, and National Ave.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_basemap(\"OpenStreetMap\")+\n  tm_shape(sf_sensors_raw |&gt;\n             filter(str_detect(desc,\"TSI047 COUNTY ROAD / BRICKNALL AVE\")))+\n  tm_dots(col = \"darkgreen\",\n          size = 2,\n          alpha = 0.7,\n          group = \"Selected sites\")\n\n\n\n\n\nAlso, the direction of the different counts, in some cases, refer to the specific lane where the sensors are installed. Therefore, it is useful to extract the flow direction from description.\nSome count points need to be adjusted manually: - there is no a valid description of the direction of sensor N43121Y. See this. - there is a typo (wesbound instead of westbound) in the direction of sensors N40134Fand N45131D.\n\nmy_data_expanded$direction[my_data_expanded$name == \"N43121Y\"] &lt;- \"EASTBOUND\"\nmy_data_expanded$direction[my_data_expanded$name %in% c(\"N40134F\", \"N45131D\")] &lt;- \"WESTBOUND\"\n\n\nmy_data_expanded_dir &lt;- my_data_expanded |&gt; \n  mutate(dir_str = str_to_lower(direction) |&gt; \n           str_extract(pattern = \"\\\\b\\\\w*bound\\\\b\")) |&gt; \n  mutate(origin_str = if_else(is.na(dir_str),\n                           direction |&gt; \n                             str_remove(\"TRAVEL{1,3}ING \") |&gt;\n                             str_remove(\"\\\\sINTENDING\") |&gt;\n                             str_extract(\"\\\\w*(\\\\s)?\\\\w*\\\\b(?=\\\\sTO)\"),\n                           NA)) |&gt; \n  mutate(dir_str = case_when(is.na(dir_str)&origin_str==\"EAST\"~\"westbound\",\n                             is.na(dir_str)&origin_str==\"NORTH\"~\"southbound\",\n                             is.na(dir_str)&origin_str==\"SOUTH WEST\"~\"northeastbound\",\n                             is.na(dir_str)&origin_str==\"SOUTH\"~\"northbound\",\n                             is.na(dir_str)&origin_str==\"WEST\"~\"eastbound\",\n                             is.na(dir_str)&origin_str==\"SOUTH NORTH\"~\"northbound\",\n                             is.na(dir_str)&str_detect(direction,\"FROM THE NORTH\")~\"southbound\",\n                             is.na(dir_str)&str_detect(direction,\"FROM THE SOUTH\")~\"northbound\",\n                             is.na(dir_str)&str_detect(direction,\"FROM THE EAST\")~\"westbound\",\n                             is.na(dir_str)&str_detect(direction,\"FROM THE WEST\")~\"eastbound\",\n                             TRUE ~ dir_str)) |&gt; \n  select(-origin_str)\n\n\ndir_groups &lt;- do.call(bind_rows,\n                      lapply(seq_along(sensors_overlap),\n                             function(i){\n                               tsensor &lt;-  sensors_overlap[[i]]\n                               id_group  &lt;- i\n                               tmp_group &lt;- my_data_expanded_dir[tsensor,] |&gt;\n                                 mutate(subgroup_id = cur_group_id(),\n                                        group_id = id_group,\n                                        .by = c(desc,dir_str)) |&gt;\n                                 select(name,desc,dir_str,group_id,subgroup_id)\n                               })) \n\n\nsf_counts &lt;- my_data_expanded_dir |&gt; \n  left_join(dir_groups,by = join_by(name,desc,dir_str)) |&gt; \n  summarise(across(ends_with(\"itude\"),mean),\n            .by = c(group_id,subgroup_id,desc,dir_str)) |&gt; \n  st_as_sf(\n    coords = c(\"longitude\",\"latitude\"),\n    crs = 4326) |&gt;\n  st_transform(crs = 27700)\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap(\"OpenStreetMap\")+\n  tm_shape(sf_counts) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\nDownloading data\nSince the open data platform hosting the counts data is based on CKAN, the raw CSV files can be downloaded directly with the following code (see this):\n\ndir.create(\"02_raw_data_counts\", showWarnings = F)\n\nbase_url &lt;- \"https://opendata.hullcc.gov.uk/datastore/dump/\"\nbase_path &lt;- \"02_raw_data_counts\"\n\n\n# In case files are downloaded separately \ndownloaded_files &lt;- list.files(\"02_raw_data_counts/\")\nids_download &lt;- my_data$resource_id[!(my_data$name %in% gsub(\"\\\\.csv\",\"\",x = downloaded_files))]\n\n# Loop for downloading files\nif (!identical(ids_download, character(0))) {\n  for (id in ids_download) {\n    try(download.file(\n      url = paste0(base_url, id),\n      destfile = paste0(base_path,\n                        \"/\",\n                        my_data$name[my_data$resource_id == id],\n                        \".csv\")\n    ))\n  }\n} \n\n\n\nPre-processing\nFirst, we upload the downloaded_files object with all the files available\n\ndownloaded_files_full &lt;- list.files(\"02_raw_data_counts/\",full.names = T)\n\n\nlibrary(data.table)\nlibrary(dtplyr)\n\nsetDTthreads(0)\n\nraw_data &lt;- rbindlist(lapply(downloaded_files_full,fread))\nhead(raw_data)\n\n     _id  LinkID AttributeID TravelTime VehicleFlow Speed     MeasurementTime\n   &lt;int&gt;  &lt;char&gt;       &lt;int&gt;      &lt;int&gt;       &lt;int&gt; &lt;int&gt;              &lt;POSc&gt;\n1:     1 N10111A     9601773          0           0     0 2020-02-07 15:38:00\n2:     2 N10111A     9601773          0           0    80 2020-02-07 16:49:14\n3:     3 N10111A     9601773          0           0    80 2020-02-07 16:59:14\n4:     4 N10111A     9601773          0           0    80 2020-02-07 17:04:14\n5:     5 N10111A     9601773          0           0    80 2020-02-07 17:09:14\n6:     6 N10111A     9601773         70           9    80 2020-02-07 17:19:14\n    Timestamp\n        &lt;int&gt;\n1: 1581089880\n2: 1581094154\n3: 1581094754\n4: 1581095054\n5: 1581095354\n6: 1581095954\n\n\nSome general checks of the data\nNumber of sites in the raw data:\n\nraw_data$LinkID |&gt; unique() |&gt; length()\n\n[1] 296\n\n\nRange of dates:\n\nraw_data$MeasurementTime |&gt; range()\n\n[1] \"2020-02-07 15:38:00 UTC\" \"2024-01-09 19:30:00 UTC\"\n\n\n\nCollection rate check\nA check of the number of records per site\n\nhist(raw_data |&gt;\n       summarise(records = n(),.by = LinkID) |&gt;\n       as_tibble() |&gt;\n       pull(records),breaks = seq(0,350000,1000),\n     xlab = \"Number of records\",\n     main = \"Distribution of total records per LinkID\")\n\n\n\n\n\n\n\n\nA closer look to the sites with a low number of records:\n\nlow_records_IDs &lt;- raw_data |&gt;\n  filter(between(year(MeasurementTime),2022,2023)) |&gt; \n  summarise(records = n(),.by = LinkID) |&gt;\n  filter(records &lt; 150000) |&gt; \n  as_tibble() |&gt; \n  pull(LinkID)\n\nlow_records_IDs\n\n [1] \"N10221X\" \"N30121C\" \"N48111F\" \"N48111G\" \"N48112B\" \"N48112C\" \"N48112D\"\n [8] \"N48112E\" \"N70111A\" \"N70111B\" \"N70111C\" \"N70111D\" \"N70121F\" \"N70121H\"\n[15] \"N70131B\" \"N70131D\" \"N70141E\" \"N70141F\" \"N70141H\"\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap()+\n  tm_shape(sf_sensors_raw |&gt;\n  filter(!(name %in% low_records_IDs)))+\n  tm_dots(col = \"black\",\n          alpha = 0.3,\n          group = \"Other sites\")+\n  tm_shape(sf_sensors_raw |&gt;\n             filter(name %in% low_records_IDs))+\n  tm_dots(col = \"red\",\n          group = \"Low record sites\")\n\n\n\n\n\n\n\n\n\ndaily_data &lt;- raw_data[between(year(MeasurementTime),2022,2023)\n             ][,GBtimestamp := with_tz(MeasurementTime,tzone = \"Europe/London\")\n               ][,`:=`(Date = date(GBtimestamp),\n                       Year = year(GBtimestamp))][\n                         ,.(records = .N,\n                            Flow = sum(VehicleFlow,na.rm = T)),\n                         by = .(LinkID, Date, Year)\n                       ]\n\nRecords per day per site by Year (max 288 5-minutes intervals)\n\ndaily_data[,.(records = mean(records)),\n  .(LinkID, Year)] |&gt; \nggplot(aes(records))+\n  geom_histogram(binwidth = 1,col = \"white\")+\n  facet_grid(Year~.)+\n  theme_light()\n\n\n\n\n\n\n\n\nTo identify the IDs with a low number of daily records, we run:\n\nlow_annual_IDs &lt;- unique(daily_data[,c(\"Year\",\"LinkID\",\"Date\")])[,\n                                               .(n_days = .N),\n                                               .(LinkID, Year)][n_days&lt;240] |&gt; pull(LinkID)\n\nThe IDs with low records match exactly the ones previously identified.\n\nidentical(low_annual_IDs,low_records_IDs)\n\n[1] TRUE\n\n\nThese IDs will be discarded as the available data might not be representative to produce AADF; if the ID is part of a group, the whole group will be discarded as AADF for the group can be affected.\n\ngroups_include &lt;- dir_groups |&gt; \n  mutate(not_include = (name %in% low_annual_IDs)*1) |&gt; \n  filter(sum(not_include)==0,.by = c(desc,dir_str,group_id)) |&gt;\n  select(-not_include) \n\n\n\nDaily flows sense check\n\ntotal_d_flow &lt;- daily_data |&gt; \n  inner_join(groups_include,by = c(\"LinkID\"=\"name\")) |&gt; \n  summarise(Flow = sum(Flow),.by=Date)\n\n\n  ggplot(total_d_flow,\n         aes(x=Date,y=Flow))+\n  geom_line() +\n  geom_point(data = total_d_flow |&gt;\n               filter (Flow&lt;500e3),\n             col = \"red\")+\n  geom_text(data = total_d_flow |&gt;\n               filter (Flow&lt;500e3),\n            aes(label = Date),\n             col = \"#020202\")\n\n\n\n\n\n\n\n\nThe daily flows on Christmas day are used a sensible threshold to identify outliers which will be discarded. It is assumed that the lowest traffic over the year occurs on that day.\n\nmin_Xmas &lt;- daily_data |&gt;\n  filter(day(Date)==25,month(Date)==12) |&gt; \n  summarise(Flow = sum(Flow),.by=c(Date)) |&gt; \n  pull(Flow) |&gt; \n  min()\n\ntotal_d_flow &lt;- daily_data |&gt; \n  inner_join(groups_include,by = c(\"LinkID\"=\"name\")) |&gt; \n  filter(sum(Flow)&gt;=(min_Xmas*0.99),.by=Date) |&gt; \n  summarise(Flow = sum(Flow),.by=Date)\n\n\n  ggplot(total_d_flow,\n         aes(x=Date,y=Flow))+\n  geom_line() +\n  geom_point(data = total_d_flow |&gt;\n               filter (Flow&lt;600e3),\n             col = \"red\")+\n  geom_text(data = total_d_flow |&gt;\n               filter (Flow&lt;600e3),\n            aes(label = Date),\n             col = \"#020202\")\n\n\n\n\n\n\n\n\n\n\nZero-flow sites check\n\nsite_d_flow &lt;- daily_data |&gt; \n  inner_join(groups_include,by = c(\"LinkID\"=\"name\")) |&gt; \n  filter(sum(Flow)&gt;=min_Xmas,.by=Date) |&gt; \n  summarise(Flow = sum(Flow),.by=c(Year,Date,group_id,subgroup_id)) \n\nno_flow_sites &lt;- site_d_flow |&gt;\n  summarise(Flow = sum(Flow),.by=c(group_id,subgroup_id)) |&gt;\n  filter(Flow == 0)\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap()+\n  tm_shape(sf_counts |&gt;\n             anti_join(no_flow_sites,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"black\",alpha = 0.3,group = \"Other sites\")+\n  tm_shape(sf_counts |&gt;\n             semi_join(no_flow_sites,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"red\",group = \"Zero counts sites\")\n\n\n\n\n\n\n\n\nThe following plot shows the distribution of daily counts for all sites.\n\nsite_d_flow |&gt;  \n  ggplot(aes(Flow))+\n  geom_histogram(binwidth = 100)+\n  facet_wrap(Year~.)\n\n\n\n\n\n\n\n\nConsidering that all sensors are located in signalised junctions with significant levels of traffic, zero counts are assumed to be the result of a problem with the data or non-typical traffic conditions, therefore those records are discarded.\n\n\nAnnual Average Daily Flow (AADF) calculation\n\naadf_data &lt;- site_d_flow |&gt; \n  filter(Flow&gt;0) |&gt; \n  anti_join(no_flow_sites,\n                       by = join_by(group_id,subgroup_id)) |&gt; \n  summarise(Flow = mean(Flow),.by = c(Year,group_id,subgroup_id)) |&gt; \n  pivot_wider(names_from = \"Year\",values_from = \"Flow\",names_prefix = \"flow.\")\n\n\n\n2022 vs 2023 flows high-level check\n\naadf_checks &lt;- aadf_data |&gt; \n  mutate(diff = flow.2023-flow.2022,\n         pdiff = diff/flow.2022) |&gt; \n  arrange(-abs(pdiff))\n\n\n  ggplot(data = aadf_checks,\n         aes(flow.2022,flow.2023))+\n  geom_smooth(formula = \"y ~ x+0\",method = \"lm\",se = F,alpha = 0.4)+\n  geom_point(shape = 19,alpha = 0.6)+\n  geom_point(data = aadf_checks |&gt;\n               filter(abs(pdiff)&gt;0.5|is.nan(pdiff)),\n             shape = 19,\n             alpha = 0.4,\n             size = 3,\n             col= \"red\")+\n  theme_light()+\n  coord_fixed()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nhigh_change_counts &lt;- aadf_checks |&gt;\n  filter(abs(pdiff)&gt;0.5|is.nan(pdiff))\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap()+\n  tm_shape(sf_counts |&gt;\n             anti_join(high_change_counts,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"black\",alpha = 0.3,group = \"Other sites\")+\n  tm_shape(sf_counts |&gt;\n             semi_join(high_change_counts,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"red\",group = \"High change sites\")\n\n\n\n\n\n\n\n\nAlthough these changes seem suspicious, these records will not be discarded for the final analysis. Instead, values for both years are going to be replaced by the greater record.\n\naadf_data_2 &lt;- aadf_checks |&gt;\n  mutate(fmax = pmax(flow.2022,flow.2023,na.rm = T)) |&gt; \n  mutate(flow.2022 = if_else(abs(pdiff)&gt;0.5|is.nan(pdiff)|is.na(pdiff),fmax,flow.2022),\n         flow.2023 = if_else(abs(pdiff)&gt;0.5|is.nan(pdiff)|is.na(pdiff),fmax,flow.2023)) |&gt; \n  select(group_id:flow.2023)\n\n\n\nVery low flows\n\nrange(aadf_data_2$flow.2022)\n\n[1]     3.00 20772.63\n\nrange(aadf_data_2$flow.2023)\n\n[1]     3.00 20948.87\n\n\n\nlow_aadf &lt;- aadf_data_2 |&gt; \n  mutate(across(starts_with(\"flow\"),list(low = \\(x) x&lt;100))) |&gt; \n  filter(flow.2022_low|flow.2023_low) |&gt; \n  select(group_id,subgroup_id)\n\n\ntm_basemap()+\n  tm_shape(sf_counts |&gt;\n             anti_join(low_aadf,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"black\",alpha = 0.3,group = \"Other sites\")+\n  tm_shape(sf_counts |&gt;\n             semi_join(low_aadf,\n                       by = join_by(group_id,subgroup_id)))+\n  tm_dots(col = \"red\",size = 1, group = \"Low AADF\")\n\n\n\n\n\n\n\n\nThe sites with AADF lower than 100 are discarded\n\naadf_data_clean &lt;- aadf_data_2 |&gt;\n  anti_join(low_aadf,by = join_by(group_id,subgroup_id))\n\nThe spatial object with the counts is updated to make it consistent with the AADF dataframe\n\nsf_counts_selected &lt;- sf_counts |&gt;\n  semi_join(aadf_data_clean,\n            by = join_by(group_id,subgroup_id))\n\n\n\nStudy area bounds\n\nbuffer_distance = max(st_distance(sf_counts,sf_counts))*0.1\nbuffer_distance_exp = max(st_distance(sf_counts,sf_counts))*0.5\nlibrary(MinorRoadTraffic)\nbounds_model &lt;- sf_counts |&gt;\n  make_convex_hull(dist = buffer_distance)\nbounds &lt;- sf_counts |&gt;\n  make_convex_hull(dist = buffer_distance_exp)\n\n\n\n\nSaving results\nThe following code produces a csv file with the AADF of all sites, and geoJSON files for the grouped counts and the convex hull formed by them.\n\ndir.create(\"03_preprocessing_files\",showWarnings = F)\nwrite_csv(aadf_data_clean,file = \"03_preprocessing_files/aadf_data.csv\",append = F)\n\ntry(file.remove(\"03_preprocessing_files/grouped_counts.geojson\"))\n\n[1] TRUE\n\nst_write(sf_counts_selected,\"03_preprocessing_files/grouped_counts.geojson\",append = F)\n\nWriting layer `grouped_counts' to data source \n  `03_preprocessing_files/grouped_counts.geojson' using driver `GeoJSON'\nWriting 185 features with 4 fields and geometry type Point.\n\ntry(file.remove(\"03_preprocessing_files/bounds.geoJSON\"))\n\n[1] TRUE\n\nst_write(bounds,\"03_preprocessing_files/bounds.geoJSON\")\n\nWriting layer `bounds' to data source \n  `03_preprocessing_files/bounds.geoJSON' using driver `GeoJSON'\nWriting 1 features with 0 fields and geometry type Polygon.\n\ntry(file.remove(\"03_preprocessing_files/bounds_model.geoJSON\"))\n\n[1] TRUE\n\nst_write(bounds_model,\"03_preprocessing_files/bounds_model.geoJSON\")\n\nWriting layer `bounds_model' to data source \n  `03_preprocessing_files/bounds_model.geoJSON' using driver `GeoJSON'\nWriting 1 features with 0 fields and geometry type Polygon.",
    "crumbs": [
      "Preprocessing",
      "Count Data"
    ]
  },
  {
    "objectID": "0B_Data_sources.html#road-network",
    "href": "0B_Data_sources.html#road-network",
    "title": "Data Sources",
    "section": "Road Network",
    "text": "Road Network",
    "crumbs": [
      "Estimation models",
      "Data Sources"
    ]
  },
  {
    "objectID": "0B_Data_sources.html#socio-economic-data",
    "href": "0B_Data_sources.html#socio-economic-data",
    "title": "Data Sources",
    "section": "Socio-Economic Data",
    "text": "Socio-Economic Data",
    "crumbs": [
      "Estimation models",
      "Data Sources"
    ]
  },
  {
    "objectID": "0A_Causal_model.html",
    "href": "0A_Causal_model.html",
    "title": "Causal Model",
    "section": "",
    "text": "Attaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\n\n\n\n\n\n\nWhere:\n\n\\(T\\) is the traffic flow on a minor road,\n\\(P\\) is the population living along the minor road,\n\\(E\\) employees and working owners along the minor road,\n\\(V\\) the cars available along the minor road,\n\\(M\\) the traffic flow of the nearest major road with observed traffic,\n\\(J\\) the distance to the nearest minor-major road junction,\n\\(D\\) the road density in the LSOA where the minor road is located, and\n\\(C\\) is the standardised centrality of the minor road link.",
    "crumbs": [
      "Estimation models",
      "Causal Model"
    ]
  },
  {
    "objectID": "0C_Estimator_selection.html#geographically-weighted-poisson-regression-gwpr",
    "href": "0C_Estimator_selection.html#geographically-weighted-poisson-regression-gwpr",
    "title": "Estimator Selection",
    "section": "Geographically Weighted Poisson Regression (GWPR)",
    "text": "Geographically Weighted Poisson Regression (GWPR)",
    "crumbs": [
      "Estimation models",
      "Estimator Selection"
    ]
  },
  {
    "objectID": "0C_Estimator_selection.html#bayesian",
    "href": "0C_Estimator_selection.html#bayesian",
    "title": "Estimator Selection",
    "section": "Bayesian",
    "text": "Bayesian",
    "crumbs": [
      "Estimation models",
      "Estimator Selection"
    ]
  },
  {
    "objectID": "1B_Census_data.html",
    "href": "1B_Census_data.html",
    "title": "Census data",
    "section": "",
    "text": "library(sf)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(MinorRoadTraffic)\n\nLoading the bounds produced from the grouped counts\n\nbounds &lt;- st_read(dsn = \"03_preprocessing_files/bounds.geoJSON\")\n\nReading layer `bounds' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\bounds.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 501634.1 ymin: 423660 xmax: 517322.2 ymax: 437607\nProjected CRS: OSGB36 / British National Grid\n\n\n\nif(!file.exists(\"02_raw_census/LSOA_selected.gpkg\")){\ndownload_lsoa_2021 &lt;- function(url = \"https://github.com/juanfonsecaLS1/GEOG5099_Analysis/releases/download/v0/LSOA_Dec_2021_Boundaries_Generalised_Clipped_EW_BGC_2022_5605507071095448309.geojson\",\n                              bounds){\n  \ndir.create(file.path(\"02_raw_census\",\"lsoa\"),recursive = T,showWarnings = F)\n\n  if(!file.exists(file.path(\"02_raw_census\",\"lsoa\",\"LSOA_2021.geojson\"))){\n  utils::download.file(url, destfile = file.path(\"02_raw_census\",\"lsoa\",\"LSOA_2021.geojson\"),\n                       mode = \"wb\")}\n  res = sf::read_sf(file.path(\"02_raw_census\",\"lsoa\",\"LSOA_2021.geojson\"))\n  return(res)\n}\n\nlsoa &lt;- download_lsoa_2021()\n\nlsoa_fixed &lt;- lsoa |&gt; st_make_valid()\nlsoa_selected &lt;- lsoa_fixed[bounds,] \nst_write(lsoa_selected,\"02_raw_census/LSOA_selected.gpkg\")\n\nrm(lsoa,lsoa_fixed)\n}else{\n  lsoa_selected &lt;- st_read(\"02_raw_census/LSOA_selected.gpkg\")\n}\n\nReading layer `LSOA_selected' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\02_raw_census\\LSOA_selected.gpkg' \n  using driver `GPKG'\nSimple feature collection with 259 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 488932.5 ymin: 411729 xmax: 527121.4 ymax: 446712.1\nProjected CRS: OSGB36 / British National Grid\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap(\"OpenStreetMap\")+\n  tm_shape(lsoa_selected)+\n  tm_polygons(col = \"blue\",\n              alpha = 0.3,\n              border.col = \"blue\")",
    "crumbs": [
      "Preprocessing",
      "Census data"
    ]
  },
  {
    "objectID": "1B_Census_data.html#lsoa-boundaries",
    "href": "1B_Census_data.html#lsoa-boundaries",
    "title": "Census data",
    "section": "",
    "text": "library(sf)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(MinorRoadTraffic)\n\nLoading the bounds produced from the grouped counts\n\nbounds &lt;- st_read(dsn = \"03_preprocessing_files/bounds.geoJSON\")\n\nReading layer `bounds' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\bounds.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 501634.1 ymin: 423660 xmax: 517322.2 ymax: 437607\nProjected CRS: OSGB36 / British National Grid\n\n\n\nif(!file.exists(\"02_raw_census/LSOA_selected.gpkg\")){\ndownload_lsoa_2021 &lt;- function(url = \"https://github.com/juanfonsecaLS1/GEOG5099_Analysis/releases/download/v0/LSOA_Dec_2021_Boundaries_Generalised_Clipped_EW_BGC_2022_5605507071095448309.geojson\",\n                              bounds){\n  \ndir.create(file.path(\"02_raw_census\",\"lsoa\"),recursive = T,showWarnings = F)\n\n  if(!file.exists(file.path(\"02_raw_census\",\"lsoa\",\"LSOA_2021.geojson\"))){\n  utils::download.file(url, destfile = file.path(\"02_raw_census\",\"lsoa\",\"LSOA_2021.geojson\"),\n                       mode = \"wb\")}\n  res = sf::read_sf(file.path(\"02_raw_census\",\"lsoa\",\"LSOA_2021.geojson\"))\n  return(res)\n}\n\nlsoa &lt;- download_lsoa_2021()\n\nlsoa_fixed &lt;- lsoa |&gt; st_make_valid()\nlsoa_selected &lt;- lsoa_fixed[bounds,] \nst_write(lsoa_selected,\"02_raw_census/LSOA_selected.gpkg\")\n\nrm(lsoa,lsoa_fixed)\n}else{\n  lsoa_selected &lt;- st_read(\"02_raw_census/LSOA_selected.gpkg\")\n}\n\nReading layer `LSOA_selected' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\02_raw_census\\LSOA_selected.gpkg' \n  using driver `GPKG'\nSimple feature collection with 259 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 488932.5 ymin: 411729 xmax: 527121.4 ymax: 446712.1\nProjected CRS: OSGB36 / British National Grid\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap(\"OpenStreetMap\")+\n  tm_shape(lsoa_selected)+\n  tm_polygons(col = \"blue\",\n              alpha = 0.3,\n              border.col = \"blue\")",
    "crumbs": [
      "Preprocessing",
      "Census data"
    ]
  },
  {
    "objectID": "1B_Census_data.html#downloading-data",
    "href": "1B_Census_data.html#downloading-data",
    "title": "Census data",
    "section": "Downloading Data",
    "text": "Downloading Data\nThe following code downloads the 2021 Census data.\n\nPopulation TS001\n\ndir.create(file.path(\"02_raw_census\",\"census\"),recursive = T,showWarnings = F)\nif(!file.exists(file.path(\"02_raw_census\",\n                              \"census\",\n                              \"pop_2021.zip\"))){\ndownload.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts001.zip\",\n              destfile = file.path(\"02_raw_census\",\"census\",\"pop_2021.zip\"),\n                       mode = \"wb\")}\n\npop &lt;- read_csv(unz(file.path(\"02_raw_census\",\n                              \"census\",\n                              \"pop_2021.zip\"),\n                    \"census2021-ts001-lsoa.csv\"),\n                col_types = cols(\n  date = col_double(),\n  geography = col_character(),\n  `geography code` = col_character(),\n  `Residence type: Total; measures: Value` = col_double(),\n  `Residence type: Lives in a household; measures: Value` = col_double(),\n  `Residence type: Lives in a communal establishment; measures: Value` = col_double()\n)\n) |&gt; rename_with(.cols = starts_with(\"Residence\"),\n                 ~ gsub(\"Residence type: \",\"\",.x))|&gt; \n  tibble(.name_repair = \"universal\")\n\n\nWorkplace population\n\nif(!file.exists(file.path(\"02_raw_census\",\"census\",\"wp_pop_2021.zip\"))){\n  download.file(\"https://www.nomisweb.co.uk/output/census/2021/wp001.zip\",\n              destfile = file.path(\"02_raw_census\",\"census\",\"wp_pop_2021.zip\"),\n                       mode = \"wb\")}\nwp_pop &lt;- read_csv(\n  unz(\n    file.path(\"02_raw_census\",\n              \"census\",\n              \"wp_pop_2021.zip\"),\n    \"WP001_lsoa.csv\"\n  ),\n  col_types = cols(\n    `Lower layer Super Output Areas Code` = col_character(),\n    `Lower layer Super Output Areas Label` = col_character(),\n    Count = col_double()\n  )\n) |&gt; \n  tibble(.name_repair = \"universal\")\n\n\n\nWorkplace method to travel to work\n\nif(!file.exists(file.path(\"02_raw_census\", \"census\", \"wp_mode_2021.zip\"))) {\n  download.file(\n    \"https://www.nomisweb.co.uk/output/census/2021/wp025.zip\",\n    destfile = file.path(\"02_raw_census\", \"census\", \"wp_mode_2021.zip\"),\n    mode = \"wb\"\n  )\n}\nwp_mode &lt;- read_csv(\n  unz(\n    file.path(\"02_raw_census\",\n              \"census\",\n              \"wp_mode_2021.zip\"),\n    \"WP025_msoa.csv\"\n  ),\n  col_types = cols(\n    `Middle layer Super Output Areas Code` = col_character(),\n    `Middle layer Super Output Areas Label` = col_character(),\n    `Method used to travel to workplace (12 categories) Code` = col_double(),\n    `Method used to travel to workplace (12 categories) Label` = col_character(),\n    Count = col_double()\n  )\n) |&gt; \n  tibble(.name_repair = \"universal\")\n\n\n\n\nExmployment Status TS066\n\nif(!file.exists(file.path(\"02_raw_census\",\"census\",\"employment_2021.zip\"))){\n  download.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts066.zip\",\n              destfile = file.path(\"02_raw_census\",\"census\",\"employment_2021.zip\"),\n                       mode = \"wb\")}\n\nemploy &lt;- read_csv(unz(file.path(\"02_raw_census\",\n                              \"census\",\n                              \"employment_2021.zip\"),\n                    \"census2021-ts066-lsoa.csv\")\n,\ncol_types = cols(\n  date = col_double(),\n  geography = col_character(),\n  `geography code` = col_character(),\n  `Economic activity status: Total: All usual residents aged 16 years and over` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students)` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment:Employee` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Employee: Part-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Employee: Full-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment:Self-employed with employees` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed with employees: Part-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed with employees: Full-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students):In employment:Self-employed without employees` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed without employees: Part-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): In employment: Self-employed without employees: Full-time` = col_double(),\n  `Economic activity status: Economically active (excluding full-time students): Unemployed` = col_double(),\n  `Economic activity status: Economically active and a full-time student` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment:Employee` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Employee: Part-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Employee: Full-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment:Self-employed with employees` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed with employees: Part-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed with employees: Full-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student:In employment:Self-employed without employees` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed without employees: Part-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: In employment: Self-employed without employees: Full-time` = col_double(),\n  `Economic activity status: Economically active and a full-time student: Unemployed` = col_double(),\n  `Economic activity status: Economically inactive` = col_double(),\n  `Economic activity status: Economically inactive: Retired` = col_double(),\n  `Economic activity status: Economically inactive: Student` = col_double(),\n  `Economic activity status: Economically inactive: Looking after home or family` = col_double(),\n  `Economic activity status: Economically inactive: Long-term sick or disabled` = col_double(),\n  `Economic activity status: Economically inactive: Other` = col_double()\n)\n) |&gt; rename_with(.cols = starts_with(\"Economic\"),\n                 ~ gsub(\"Economic activity status: \",\"\",.x))|&gt; \n  tibble(.name_repair = \"universal\")\n\n\n\nCar Availability TS045\n\nif(!file.exists(file.path(\"02_raw_census\",\"census\",\"car_avail_2021.zip\"))){\n  download.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts045.zip\",\n              destfile = file.path(\"02_raw_census\",\"census\",\"car_avail_2021.zip\"),\n                       mode = \"wb\")\n  }\ncar_avail &lt;- read_csv(unz(file.path(\"02_raw_census\",\n                              \"census\",\n                              \"car_avail_2021.zip\"),\n                    \"census2021-ts045-lsoa.csv\"),\n                    col_types = cols(\n  date = col_double(),\n  geography = col_character(),\n  `geography code` = col_character(),\n  `Number of cars or vans: Total: All households` = col_double(),\n  `Number of cars or vans: No cars or vans in household` = col_double(),\n  `Number of cars or vans: 1 car or van in household` = col_double(),\n  `Number of cars or vans: 2 cars or vans in household` = col_double(),\n  `Number of cars or vans: 3 or more cars or vans in household` = col_double()\n)\n) |&gt; rename_with(.cols = starts_with(\"Number\"),\n                 ~ gsub(\"Number of cars or vans: \",\"\",.x))|&gt; \n  tibble(.name_repair = \"universal\")\n\n\n\nMethod to work (mode) TS061\n\nif(!file.exists(file.path(\"02_raw_census\",\"census\",\"comm_mode_2021.zip\"))){\n  download.file(\"https://www.nomisweb.co.uk/output/census/2021/census2021-ts061.zip\",\n              destfile = file.path(\"02_raw_census\",\"census\",\"comm_mode_2021.zip\"),\n                       mode = \"wb\")\n  }\ncomm_mode &lt;- read_csv(\n  unz(file.path(\"02_raw_census\",\n                \"census\",\n                \"comm_mode_2021.zip\"),\n      \"census2021-ts061-lsoa.csv\",),\n  col_types = cols(\n    date = col_double(),\n    geography = col_character(),\n    `geography code` = col_character(),\n    `Method of travel to workplace: Total: All usual residents aged 16 years and over in employment the week before the census` = col_double(),\n    `Method of travel to workplace: Work mainly at or from home` = col_double(),\n    `Method of travel to workplace: Underground, metro, light rail, tram` = col_double(),\n    `Method of travel to workplace: Train` = col_double(),\n    `Method of travel to workplace: Bus, minibus or coach` = col_double(),\n    `Method of travel to workplace: Taxi` = col_double(),\n    `Method of travel to workplace: Motorcycle, scooter or moped` = col_double(),\n    `Method of travel to workplace: Driving a car or van` = col_double(),\n    `Method of travel to workplace: Passenger in a car or van` = col_double(),\n    `Method of travel to workplace: Bicycle` = col_double(),\n    `Method of travel to workplace: On foot` = col_double(),\n    `Method of travel to workplace: Other method of travel to work` = col_double()\n  )\n  ) |&gt;\n  rename_with(.cols = starts_with(\"Method\"),\n              ~ gsub(\"Method of travel to workplace: \",\"\",.x)) |&gt; \n  tibble(.name_repair = \"universal\")\n\n\n\nData from Place-based Carbon Calculator\n\npbcc &lt;- download_pbcc() |&gt;\n  semi_join(lsoa_selected,by = c(\"LSOA11NM\"=\"LSOA21NM\"))",
    "crumbs": [
      "Preprocessing",
      "Census data"
    ]
  },
  {
    "objectID": "1B_Census_data.html#pre-processing",
    "href": "1B_Census_data.html#pre-processing",
    "title": "Census data",
    "section": "Pre-processing",
    "text": "Pre-processing\n\nSub-setting the census data\n\nPopulation\n\nlsoa_pop &lt;- pop |&gt;\n  semi_join(lsoa_selected, by = c(\"geography.code\"=\"LSOA21CD\")) |&gt;\n  select(geography.code,Total..measures..Value) |&gt; \n  rename(total_pop = Total..measures..Value)\n\n\n\nEmployment status\n\nlsoa_employ &lt;- employ |&gt;\n  semi_join(lsoa_selected, by = c(\"geography.code\"=\"LSOA21CD\")) |&gt;\n  select(geography.code, Economically.active..excluding.full.time.students..In.employment) |&gt; \n  rename(total_employed = Economically.active..excluding.full.time.students..In.employment)\n\n\n\nWorkplace population\n\nlsoa_wp_pop &lt;- wp_pop |&gt; \n  semi_join(lsoa_selected,\n            by = c(\"Lower.layer.Super.Output.Areas.Code\"=\"LSOA21CD\")) |&gt; \n  select(Lower.layer.Super.Output.Areas.Code,Count) |&gt; \n  rename(geography.code = Lower.layer.Super.Output.Areas.Code,\n         wk_pop = Count)\n\n\n\nCar Availability\n\nlsoa_car_avail &lt;- car_avail |&gt;\n  semi_join(lsoa_selected, by = c(\"geography.code\"=\"LSOA21CD\")) |&gt;\n  mutate(car_avail_perc = (Total..All.households-No.cars.or.vans.in.household)/Total..All.households) |&gt; \n  select(geography.code,car_avail_perc)\n\n\n\nMethot to travel to work\n\nlsoa_comm_mode &lt;- comm_mode |&gt;\n  semi_join(lsoa_selected, by = c(\"geography.code\"=\"LSOA21CD\")) |&gt; \n  mutate(car_comm_perc = (Taxi+Driving.a.car.or.van+Passenger.in.a.car.or.van)/Total..All.usual.residents.aged.16.years.and.over.in.employment.the.week.before.the.census) |&gt; \n  select(geography.code,car_comm_perc)\n\n\n\nCarbon Calculator\n\nlsoa_pbcc &lt;- pbcc |&gt; select(LSOA11,cars_percap_2018)\n\n\n\n\nConsolidating and saving Results\n\nLSOA_data &lt;- lsoa_selected |&gt; \n  left_join(\n    lsoa_pop |&gt;\n      left_join(lsoa_employ, by = \"geography.code\") |&gt;\n      left_join(lsoa_wp_pop, by = \"geography.code\") |&gt;\n      left_join(lsoa_car_avail, by = \"geography.code\")  |&gt;\n      left_join(lsoa_comm_mode, by = \"geography.code\") |&gt;\n      left_join(lsoa_pbcc, by = c(\"geography.code\" = \"LSOA11\")), by = c(\"LSOA21CD\"=\"geography.code\"))\n\n\nst_write(LSOA_data,\"03_preprocessing_files/LSOA_data.gpkg\",append = F)\n\nDeleting layer `LSOA_data' using driver `GPKG'\nWriting layer `LSOA_data' to data source \n  `03_preprocessing_files/LSOA_data.gpkg' using driver `GPKG'\nWriting 259 features with 10 fields and geometry type Multi Polygon.\n\n\n\n\nSome visualisations of the data\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(LSOA_data)+\n  tm_polygons(col = \"total_pop\",\n              border.col = \"white\",\n              palette = \"Blues\",\n              title = \"Total Population 2021\")\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(LSOA_data)+\n  tm_polygons(col = \"wk_pop\",\n              border.col = \"white\",\n              style = \"jenks\",\n              palette = \"Reds\",\n              title = \"Total Workplace Population 2021\")\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(LSOA_data)+\n  tm_polygons(col = \"total_employed\",\n              border.col = \"white\",\n              palette = \"Greens\",\n              title = \"Total people Employed 2021\")\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(LSOA_data)+\n  tm_polygons(col = \"car_avail_perc\",\n              border.col = \"white\",\n              palette = \"Reds\",\n              title = \"% of households with one or more cars\")\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(LSOA_data)+\n  tm_polygons(col = \"car_comm_perc\",\n              border.col = \"white\",\n              palette = \"YlGn\",\n              title = \"Car commuting split %\")\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_basemap(\"Esri.WorldGrayCanvas\")+\ntm_shape(LSOA_data)+\n  tm_polygons(col = \"cars_percap_2018\",\n              border.col = \"white\",\n              palette = c(\"yellow\", \"darkgreen\"),\n              title = \"Cars per Capita 2018\")",
    "crumbs": [
      "Preprocessing",
      "Census data"
    ]
  },
  {
    "objectID": "1D_EDA.html",
    "href": "1D_EDA.html",
    "title": "Data Consolidation and Exploration",
    "section": "",
    "text": "library(tidyverse)\nlibrary(kableExtra)\nlibrary(hrbrthemes)\nlibrary(tmap)\nlibrary(sf)",
    "crumbs": [
      "Preprocessing",
      "Data Consolidation and Exploration"
    ]
  },
  {
    "objectID": "1D_EDA.html#loading-data",
    "href": "1D_EDA.html#loading-data",
    "title": "Data Consolidation and Exploration",
    "section": "Loading data",
    "text": "Loading data\n\nnet_data_sf &lt;- st_read(\"03_preprocessing_files/network_data.gpkg\", layer = \"network_data\")\n\nReading layer `network_data' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\network_data.gpkg' \n  using driver `GPKG'\nSimple feature collection with 46294 features and 32 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 494301.7 ymin: 413484.8 xmax: 523983.6 ymax: 445801.9\nProjected CRS: OSGB36 / British National Grid\n\nLSOA_sf &lt;- st_read(\"03_preprocessing_files/network_data.gpkg\", layer = \"LSOA\")\n\nReading layer `LSOA' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\network_data.gpkg' \n  using driver `GPKG'\nSimple feature collection with 259 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 488932.5 ymin: 411729 xmax: 527121.4 ymax: 446712.1\nProjected CRS: OSGB36 / British National Grid\n\njct_sf &lt;- st_read(\"03_preprocessing_files/network_data.gpkg\", layer = \"junctions\")\n\nReading layer `junctions' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\network_data.gpkg' \n  using driver `GPKG'\nSimple feature collection with 17874 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.5694518 ymin: 53.60775 xmax: -0.1200969 ymax: 53.89554\nGeodetic CRS:  WGS 84\n\nbounds_model &lt;- st_read(\"03_preprocessing_files/bounds_model.geoJSON\")\n\nReading layer `bounds_model' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\bounds_model.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 504809.6 ymin: 426835.9 xmax: 514146.8 ymax: 434430.8\nProjected CRS: OSGB36 / British National Grid\n\nLSOA_data &lt;- LSOA_sf |&gt; st_drop_geometry()\n\nLSOA data is joined with the network data. Also, total population and total people employed are scaled based on the length proportion calculated before. Road type is converted to factor",
    "crumbs": [
      "Preprocessing",
      "Data Consolidation and Exploration"
    ]
  },
  {
    "objectID": "1D_EDA.html#joinig-lsoa-data-and-network-data",
    "href": "1D_EDA.html#joinig-lsoa-data-and-network-data",
    "title": "Data Consolidation and Exploration",
    "section": "Joinig LSOA data and network data",
    "text": "Joinig LSOA data and network data\n\nmodel_data_sf &lt;- net_data_sf[bounds_model,] |&gt;\n  left_join(LSOA_data |&gt;select(\n                LSOA21CD,\n                total_pop,\n                total_employed,\n                wk_pop,\n                car_avail_perc,\n                road_density,\n                cars_percap_2018,\n                area_km2\n              ) |&gt;\n              rename(total_emp_pop = total_employed),\n            by = join_by(LSOA21CD)) |&gt;\n  mutate(across(total_pop:wk_pop,\n                \\(x) x * portion_lsoa))\n\nA version without the geometry of the dataset\n\nmodel_data &lt;- model_data_sf |&gt;\n  sf::st_drop_geometry()",
    "crumbs": [
      "Preprocessing",
      "Data Consolidation and Exploration"
    ]
  },
  {
    "objectID": "1D_EDA.html#exploration",
    "href": "1D_EDA.html#exploration",
    "title": "Data Consolidation and Exploration",
    "section": "Exploration",
    "text": "Exploration\n\nSample size\nExtracting sample size based on the number of monitored edges\n\nmodel_data |&gt;\n  mutate(bool.flow.2023 = !is.na(flow.2023)) |&gt; \n  summarise(n_edges = n(),.by = c(bool.flow.2023,road_type)) |&gt;\n  mutate(sample_size = n_edges/sum(n_edges),\n         .by = road_type) |&gt; \n  filter(bool.flow.2023) |&gt; \n  select(road_type,sample_size) |&gt; \n  mutate(sample_size = scales::percent(sample_size)) |&gt; \n  kable(digits = 2) |&gt; \n  kable_minimal() |&gt; \n  kable_paper()\n\n\n\n\n\nroad_type\nsample_size\n\n\n\n\nminor\n0.2%\n\n\nmajor\n5.7%\n\n\n\n\n\n\n\n\nSame analysis with km of build roads (%)\n\nmodel_data |&gt;\n  mutate(bool.flow.2023 = !is.na(flow.2023)) |&gt; \n  summarise(d_edges = sum(d),.by = c(bool.flow.2023,road_type)) |&gt;\n  mutate(sample_size = d_edges/sum(d_edges),\n         .by = road_type) |&gt; \n  filter(bool.flow.2023) |&gt; \n  select(road_type,sample_size) |&gt; \n  mutate(sample_size = scales::percent(sample_size)) |&gt; \n  kable(digits = 2) |&gt; \n  kable_minimal() |&gt; \n  kable_paper()\n\n\n\n\n\nroad_type\nsample_size\n\n\n\n\nminor\n0.3%\n\n\nmajor\n7.2%\n\n\n\n\n\n\n\n\nAnalysis of junction types with flows\n\njct_sf|&gt;\n  sf::st_drop_geometry() |&gt; \n  mutate(bool.flow.2023 = !is.na(flow.2023)) |&gt; \n  summarise(n_jct = n(),.by = c(bool.flow.2023,jct_type)) |&gt;\n  mutate(sample_size = n_jct/sum(n_jct),\n         .by = jct_type) |&gt; \n  filter(bool.flow.2023) |&gt; \n  select(jct_type,sample_size) |&gt; mutate(sample_size = scales::percent(sample_size)) |&gt; \n  kable(digits = 2) |&gt; \n  kable_minimal() |&gt; \n  kable_paper()\n\n\n\n\n\njct_type\nsample_size\n\n\n\n\nminmaj\n4.1%\n\n\nminor\n0.1%\n\n\nmajor\n2.5%\n\n\n\n\n\n\n\n\n\n\nCorrelation between flows and some of the variables\nCentrality vs Flows\n\nmodel_data |&gt; \n  ggplot(aes(x=std.centrality,y=flow.2023,col=highway))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)+\n  theme_ipsum_rc()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 14918 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 14918 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\nPop with car availability vs flow\n\nmodel_data |&gt; \n  ggplot(aes(x=(total_pop*car_avail_perc),\n             y=flow.2023,\n             col=highway))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)+\n  theme_ipsum_rc()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 14918 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 14918 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\nEmployed Pop with car availability vs flow\n\nmodel_data |&gt; \n  ggplot(aes(x=(total_emp_pop*car_avail_perc),\n             y=flow.2023,\n             col=highway))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)+\n  theme_ipsum_rc()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 14918 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 14918 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\nWorkplace Pop vs flow\n\nmodel_data |&gt; \n  ggplot(aes(x=wk_pop,\n             y=flow.2023,\n             col=road_type))+\n  geom_point()+\n  scale_x_log10()+\n  geom_smooth(method = \"lm\",se = F)+\n  theme_ipsum_rc()\n\nWarning in scale_x_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 15055 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 14918 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nCatchment of each node\nAlternatively, population and jobs serviced can be used as as per Selby 2011. Using population and jobs served within specific distances. For this purpose, the catchmets are loaded\n\nisochrones &lt;- read_csv(\"03_preprocessing_files/junctions_catchment.csv\")\n\nRows: 4886490 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): from, tlim, id, x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis iterates through all origin nodes and calculates the total population and jobs for each time band\n\nconsolidated_catchment &lt;- isochrones |&gt;\n  mutate(id = as.character(id)) |&gt; \n  left_join(model_data |&gt;\n              select(to_id, total_pop, total_emp_pop, wk_pop),\n            by=c(\"id\"=\"to_id\"),\n            relationship = \"many-to-many\") |&gt; \n  summarise(across(ends_with(\"pop\"),\n                   \\(x) sum(x,na.rm = T)),\n            .by = c(from,tlim)) |&gt;\n  right_join(isochrones |&gt; \n                 expand(from,tlim),\n             by = c(\"from\",\"tlim\")) |&gt; \n  arrange(from,tlim) |&gt; \n  mutate(across(ends_with(\"pop\"),\n                   \\(x) if_else(is.na(x),0,x)),\n         across(ends_with(\"pop\"),\n                   list(c = \\(x) cumsum(x))),\n            .by = from) |&gt; \n  select(from,tlim,ends_with(\"c\")) |&gt; \n  pivot_wider(names_from = tlim,values_from = total_pop_c:wk_pop_c) |&gt; \n  mutate(from = as.character(from))\n\nCatchment of residents within 2 minutes\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(model_data_sf)+\n  tm_lines(\"skyblue\",lwd = 0.7)+\n  tm_shape(model_data_sf |&gt;\n  left_join(consolidated_catchment,\n            by = c(\"from_id\"=\"from\")) |&gt; \n  filter(!is.na(total_pop_c_2)))+\n  tm_lines(\"total_pop_c_2\",lwd = 2)\n\n\n\n\n\n\n\n\nCatchment of employed residents within 2 minutes\n\ntm_shape(model_data_sf)+\n  tm_lines(\"skyblue\",lwd = 0.7)+\n  tm_shape(model_data_sf |&gt;\n  left_join(consolidated_catchment,\n            by = c(\"from_id\"=\"from\")) |&gt; \n  filter(!is.na(total_emp_pop_c_2)))+\n  tm_lines(\"total_emp_pop_c_2\",lwd = 2)\n\n\n\n\n\n\n\n\nCatchment of usual workplace population\n\ntm_shape(model_data_sf)+\n  tm_lines(\"skyblue\",lwd = 0.7)+\n  tm_shape(model_data_sf |&gt;\n  left_join(consolidated_catchment,\n            by = c(\"from_id\"=\"from\")) |&gt; \n  filter(!is.na(wk_pop_c_2)))+\n  tm_lines(\"wk_pop_c_2\",lwd = 2)",
    "crumbs": [
      "Preprocessing",
      "Data Consolidation and Exploration"
    ]
  },
  {
    "objectID": "1D_EDA.html#consolidating-the-dataset-for-model-fitting",
    "href": "1D_EDA.html#consolidating-the-dataset-for-model-fitting",
    "title": "Data Consolidation and Exploration",
    "section": "Consolidating the dataset for model fitting",
    "text": "Consolidating the dataset for model fitting\nJoining the catchment to the model_data dataset\n\nmodel_data_expanded &lt;- model_data_sf |&gt;\n  left_join(consolidated_catchment,\n            by = c(\"from_id\"=\"from\")) |&gt;\n  # select(edge_id,\n  #        flow.2022,\n  #        flow.2023,\n  #        centrality,\n  #        car_avail_perc,\n  #        total_pop_c_1:wk_pop_c_5,\n  #        road_density) |&gt;\n  mutate(across(contains(\"pop\"),\\(x) x/1e3),\n         centrality = centrality,\n         across(starts_with(\"flow.\"),\\(x) round(x) |&gt; as.integer()))\n\n\nExploring interactions among the variables in the dataset\n\nmodel_data_expanded |&gt;\n  st_drop_geometry() |&gt; \n  select(flow.2023, std.centrality, total_pop_c_2,total_emp_pop_c_2, wk_pop_c_2) |&gt;\n  drop_na(flow.2023, std.centrality, total_pop_c_2,total_emp_pop_c_2, wk_pop_c_2) |&gt;\n  # mutate(across(everything(),log)) |&gt; \n  GGally::ggpairs()\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2",
    "crumbs": [
      "Preprocessing",
      "Data Consolidation and Exploration"
    ]
  },
  {
    "objectID": "1D_EDA.html#saving-data",
    "href": "1D_EDA.html#saving-data",
    "title": "Data Consolidation and Exploration",
    "section": "Saving data",
    "text": "Saving data\n\nst_write(model_data_expanded,\"03_preprocessing_files/model_data.gpkg\",delete_dsn = T) \n\nDeleting source `03_preprocessing_files/model_data.gpkg' using driver `GPKG'\nWriting layer `model_data' to data source \n  `03_preprocessing_files/model_data.gpkg' using driver `GPKG'\nWriting 15076 features with 54 fields and geometry type Line String.",
    "crumbs": [
      "Preprocessing",
      "Data Consolidation and Exploration"
    ]
  },
  {
    "objectID": "2B_Mod_eval.html",
    "href": "2B_Mod_eval.html",
    "title": "Model Evaluation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(GWmodel)\n\nLoading required package: robustbase\nLoading required package: sp\nLoading required package: Rcpp\nWelcome to GWmodel version 2.3-2.\n\nlibrary(brms)\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\nAttaching package: 'brms'\n\nThe following object is masked from 'package:robustbase':\n\n    epilepsy\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE",
    "crumbs": [
      "Modelling",
      "Model Evaluation"
    ]
  },
  {
    "objectID": "2B_Mod_eval.html#load-model-data",
    "href": "2B_Mod_eval.html#load-model-data",
    "title": "Model Evaluation",
    "section": "Load model data",
    "text": "Load model data\n\nData\n\nmodel_data_expanded &lt;- st_read(\"03_preprocessing_files/model_data.gpkg\")\n\nReading layer `model_data' from data source \n  `C:\\Users\\ts18jpf\\OneDrive - University of Leeds\\02_MsC\\99_GEOG5099M_Dissertation\\GEOG5099_Analysis\\03_preprocessing_files\\model_data.gpkg' \n  using driver `GPKG'\nSimple feature collection with 15076 features and 21 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 504496.7 ymin: 426324.9 xmax: 514189.2 ymax: 434477.1\nProjected CRS: OSGB36 / British National Grid\n\nmodel_data_noNAs&lt;- model_data_expanded |&gt; \n  select(flow.2022,\n         flow.2023,\n         centrality,\n         car_avail_perc,\n         total_pop_c_1:wk_pop_c_5,\n         road_density) |&gt;\n  drop_na()\n\n\n\nFitted models\n\nload(\"03_preprocessing_files/fitted_models.RData\")",
    "crumbs": [
      "Modelling",
      "Model Evaluation"
    ]
  },
  {
    "objectID": "2B_Mod_eval.html#model-comparison",
    "href": "2B_Mod_eval.html#model-comparison",
    "title": "Model Evaluation",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nGLM\n\nlapply(GLM_models,\n       function(mymodel){\n         summary(mymodel)\n         }\n       )\n\n[[1]]\n\nCall:\nglm(formula = myformula, family = poisson(), data = model_data_noNAs)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    8.5008357  0.0052873 1607.78   &lt;2e-16 ***\ntotal_pop_c_1 -0.1078251  0.0007680 -140.40   &lt;2e-16 ***\nwk_pop_c_1     0.0495180  0.0010195   48.57   &lt;2e-16 ***\nroad_density   0.0205803  0.0001669  123.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 293492  on 132  degrees of freedom\nResidual deviance: 246681  on 129  degrees of freedom\nAIC: 248087\n\nNumber of Fisher Scoring iterations: 5\n\n\n[[2]]\n\nCall:\nglm(formula = myformula, family = poisson(), data = model_data_noNAs)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    8.5457282  0.0055176  1548.8   &lt;2e-16 ***\ntotal_pop_c_2 -0.0319359  0.0002495  -128.0   &lt;2e-16 ***\nwk_pop_c_2     0.0154428  0.0003703    41.7   &lt;2e-16 ***\nroad_density   0.0194783  0.0001731   112.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 293492  on 132  degrees of freedom\nResidual deviance: 250182  on 129  degrees of freedom\nAIC: 251588\n\nNumber of Fisher Scoring iterations: 5\n\n\n[[3]]\n\nCall:\nglm(formula = myformula, family = poisson(), data = model_data_noNAs)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    8.6566663  0.0065606 1319.50   &lt;2e-16 ***\ntotal_pop_c_3 -0.0168852  0.0001541 -109.59   &lt;2e-16 ***\nwk_pop_c_3     0.0048641  0.0002416   20.13   &lt;2e-16 ***\nroad_density   0.0179846  0.0001812   99.24   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 293492  on 132  degrees of freedom\nResidual deviance: 255385  on 129  degrees of freedom\nAIC: 256791\n\nNumber of Fisher Scoring iterations: 5\n\n\n[[4]]\n\nCall:\nglm(formula = myformula, family = poisson(), data = model_data_noNAs)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    8.7813293  0.0079716 1101.57   &lt;2e-16 ***\ntotal_pop_c_4 -0.0131026  0.0001226 -106.86   &lt;2e-16 ***\nwk_pop_c_4     0.0064606  0.0001788   36.14   &lt;2e-16 ***\nroad_density   0.0142349  0.0001928   73.85   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 293492  on 132  degrees of freedom\nResidual deviance: 255149  on 129  degrees of freedom\nAIC: 256555\n\nNumber of Fisher Scoring iterations: 5\n\n\n[[5]]\n\nCall:\nglm(formula = myformula, family = poisson(), data = model_data_noNAs)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    8.7657205  0.0091162  961.55   &lt;2e-16 ***\ntotal_pop_c_5 -0.0088224  0.0001062  -83.09   &lt;2e-16 ***\nwk_pop_c_5     0.0032043  0.0001404   22.82   &lt;2e-16 ***\nroad_density   0.0162041  0.0001981   81.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 293492  on 132  degrees of freedom\nResidual deviance: 260928  on 129  degrees of freedom\nAIC: 262334\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nGLM_coef &lt;- lapply(GLM_models,\n       function(mymodel){\n         beta &lt;- coef(mymodel)\n         exp(confint(mymodel,level = 0.95))\n       })\n\nWaiting for profiling to be done...\nWaiting for profiling to be done...\nWaiting for profiling to be done...\nWaiting for profiling to be done...\nWaiting for profiling to be done...\n\nGLM_coef\n\n[[1]]\n                     2.5 %       97.5 %\n(Intercept)   4868.1426366 4970.0921446\ntotal_pop_c_1    0.8964337    0.8991365\nwk_pop_c_1       1.0486661    1.0528654\nroad_density     1.0204598    1.0211275\n\n[[2]]\n                     2.5 %       97.5 %\n(Intercept)   5089.3745376 5200.6504766\ntotal_pop_c_2    0.9680951    0.9690423\nwk_pop_c_2       1.0148257    1.0163000\nroad_density     1.0193236    1.0200153\n\n[[3]]\n                     2.5 %       97.5 %\n(Intercept)   5674.8814597 5822.7151361\ntotal_pop_c_3    0.9829597    0.9835535\nwk_pop_c_3       1.0044002    1.0053520\nroad_density     1.0177858    1.0185091\n\n[[4]]\n                     2.5 %       97.5 %\n(Intercept)   6410.5522822 6614.0335851\ntotal_pop_c_4    0.9867457    0.9872201\nwk_pop_c_4       1.0061290    1.0068343\nroad_density     1.0139537    1.0147201\n\n[[5]]\n                     2.5 %       97.5 %\n(Intercept)   6297.1054156 6526.2017673\ntotal_pop_c_5    0.9910101    0.9914227\nwk_pop_c_5       1.0029334    1.0034856\nroad_density     1.0159417    1.0167308\n\n\n\nGLM_coef_df &lt;- do.call(rbind,lapply(GLM_coef,\\(x){\n  y &lt;- as.data.frame(x)\n  y$id &lt;- row.names(x)\n  y$catchment &lt;- y |&gt; mutate(catchid=str_extract(id,\"c_\\\\d\")) |&gt; drop_na() |&gt; pull(catchid) |&gt; unique()\n  tibble(y) |&gt; mutate(id = str_remove(id,paste0(\"_\",catchment)))\n}))\n\n\nggplot(GLM_coef_df)+\n  geom_linerange(aes(xmin =`2.5 %`,xmax = `97.5 %`, y = catchment))+\n  facet_grid(.~id,scales = \"free\")\n\n\n\n\n\n\n\n\n\nRMSE\n\nGLM_RMSE &lt;- vapply(\n  GLM_models,\n  function(mymodel) {\n    mean((mymodel$data$flow.2023 - mymodel$fitted.values) ^ 2) ^ 0.5\n  },\n  numeric(1))\n\n\n\nAICc\n\nGLM_AICc &lt;- vapply(GLM_models,\n                   function(mymodel) {\n                     extractAIC(mymodel)[2]\n                   },\n                   numeric(1)) \n\n\n\n\nGWR\n\nGWPR_coef &lt;- lapply(GWPR_models,\n                    function(mymodel){\n                      # table of GWR coefficients\n                      t1 = exp(apply(mymodel$SDF@data[, 1:4], 2, summary)) |&gt; t()\n                      })\n\n\nGWPR_coef_df &lt;- do.call(rbind,lapply(GWPR_coef,\\(x){\n  y &lt;- as.data.frame(x)\n  y$id &lt;- row.names(x)\n  y$catchment &lt;- y |&gt; mutate(catchid=str_extract(id,\"c_\\\\d\")) |&gt; drop_na() |&gt; pull(catchid) |&gt; unique()\n  tibble(y) |&gt; mutate(id = str_remove(id,paste0(\"_\",catchment)))\n}))\n\n\nggplot(GWPR_coef_df)+\n  geom_linerange(aes(xmin =`1st Qu.`,xmax = `3rd Qu.`, y = catchment))+\n  facet_grid(.~id,scales = \"free\")\n\n\n\n\n\n\n\n\n\nRMSE\n\nGWPR_RMSE &lt;- vapply(\n  GWPR_models,\n  function(mymodel) {\n    mean((mymodel$SDF$y - mymodel$SDF$yhat) ^ 2) ^ 0.5\n  },\n  numeric(1))\n\n\n\nAICc\n\nGWPR_AICc &lt;- vapply(GWPR_models,\n                   function(mymodel) {\n                     mymodel$glms$aic\n                   },\n                   numeric(1)) \n\n\n\n\nBayes Model\n\nlibrary(posterior)\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nlapply(Bayes_NBin_models,\n       function(mymodel){\n         mymodel |&gt; as_draws_array() |&gt; summarise_draws()\n       })\n\n[[1]]\n# A tibble: 8 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept  8.49e+0  8.48e+0 0.260   0.261    8.08e+0  8.93e+0  1.00    5615.\n2 b_total_po… -1.14e-1 -1.14e-1 0.0393  0.0402  -1.78e-1 -5.03e-2  1.00    4539.\n3 b_wk_pop_c…  5.88e-2  5.68e-2 0.0585  0.0596  -3.54e-2  1.56e-1  1.00    5070.\n4 b_road_den…  2.13e-2  2.14e-2 0.00832 0.00846  7.50e-3  3.50e-2  1.00    5470.\n5 shape        2.71e+0  2.70e+0 0.321   0.326    2.20e+0  3.25e+0  1.00    4481.\n6 Intercept    8.87e+0  8.87e+0 0.0543  0.0551   8.78e+0  8.96e+0  1.00    5184.\n7 lprior      -4.70e+0 -4.70e+0 0.153   0.156   -4.94e+0 -4.44e+0  1.00    4492.\n8 lp__        -1.29e+3 -1.29e+3 1.65    1.42    -1.29e+3 -1.29e+3  1.00    1683.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n[[2]]\n# A tibble: 8 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept  8.53e+0  8.53e+0 0.262   0.258    8.10e+0  8.97e+0  1.00    4962.\n2 b_total_po… -3.45e-2 -3.44e-2 0.0133  0.0133  -5.63e-2 -1.26e-2  1.00    5010.\n3 b_wk_pop_c…  1.83e-2  1.83e-2 0.0197  0.0195  -1.39e-2  5.07e-2  1.00    4562.\n4 b_road_den…  2.03e-2  2.03e-2 0.00844 0.00836  6.57e-3  3.42e-2  1.00    4600.\n5 shape        2.69e+0  2.67e+0 0.315   0.305    2.19e+0  3.23e+0  1.00    4626.\n6 Intercept    8.87e+0  8.87e+0 0.0536  0.0523   8.78e+0  8.96e+0  1.00    4071.\n7 lprior      -4.68e+0 -4.68e+0 0.150   0.147   -4.93e+0 -4.43e+0  1.00    4629.\n8 lp__        -1.29e+3 -1.29e+3 1.57    1.44    -1.29e+3 -1.29e+3  1.00    1848.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n[[3]]\n# A tibble: 8 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept  8.63e+0  8.63e+0 0.336   0.341    8.08e+0  9.19e+0  1.00    4645.\n2 b_total_po… -1.76e-2 -1.75e-2 0.00851 0.00850 -3.14e-2 -3.75e-3  1.00    4522.\n3 b_wk_pop_c…  5.94e-3  5.90e-3 0.0127  0.0131  -1.49e-2  2.62e-2  1.00    4754.\n4 b_road_den…  1.92e-2  1.92e-2 0.00907 0.00885  4.13e-3  3.37e-2  1.00    4706.\n5 shape        2.63e+0  2.62e+0 0.317   0.315    2.13e+0  3.18e+0  1.00    4534.\n6 Intercept    8.87e+0  8.87e+0 0.0548  0.0554   8.78e+0  8.96e+0  1.00    4446.\n7 lprior      -4.66e+0 -4.66e+0 0.155   0.155   -4.91e+0 -4.40e+0  1.00    4537.\n8 lp__        -1.29e+3 -1.29e+3 1.61    1.43    -1.29e+3 -1.29e+3  1.00    1828.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n[[4]]\n# A tibble: 8 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept  8.72e+0  8.72e+0 0.410   0.404    8.06e+0  9.41e+0  1.00    4223.\n2 b_total_po… -1.26e-2 -1.27e-2 0.00661 0.00660 -2.36e-2 -1.79e-3  1.00    4537.\n3 b_wk_pop_c…  6.27e-3  6.26e-3 0.00889 0.00877 -8.31e-3  2.06e-2  1.00    4447.\n4 b_road_den…  1.62e-2  1.64e-2 0.00968 0.00969 -2.25e-5  3.17e-2  1.00    4206.\n5 shape        2.64e+0  2.62e+0 0.312   0.309    2.15e+0  3.18e+0  1.00    4353.\n6 Intercept    8.87e+0  8.87e+0 0.0537  0.0527   8.79e+0  8.96e+0  1.00    4777.\n7 lprior      -4.66e+0 -4.66e+0 0.152   0.152   -4.91e+0 -4.41e+0  1.00    4355.\n8 lp__        -1.29e+3 -1.29e+3 1.61    1.43    -1.29e+3 -1.29e+3  1.00    1799.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n[[5]]\n# A tibble: 8 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept  8.69e+0  8.68e+0 0.478   0.476    7.92e+0  9.48e+0  1.00    4579.\n2 b_total_po… -7.84e-3 -7.71e-3 0.00552 0.00545 -1.69e-2  1.11e-3  1.00    4206.\n3 b_wk_pop_c…  2.49e-3  2.50e-3 0.00701 0.00709 -9.08e-3  1.38e-2  1.00    5405.\n4 b_road_den…  1.85e-2  1.87e-2 0.0103  0.0104   1.77e-3  3.49e-2  1.00    4272.\n5 shape        2.60e+0  2.59e+0 0.304   0.298    2.13e+0  3.12e+0  1.00    4334.\n6 Intercept    8.88e+0  8.87e+0 0.0541  0.0533   8.79e+0  8.97e+0  1.00    4706.\n7 lprior      -4.64e+0 -4.64e+0 0.150   0.148   -4.88e+0 -4.39e+0  1.00    4340.\n8 lp__        -1.29e+3 -1.29e+3 1.59    1.38    -1.29e+3 -1.29e+3  1.00    1967.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\nlapply(Bayes_NBin_models,plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\n[[1]][[1]]\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]][[1]]\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]][[1]]\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]][[1]]\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]][[1]]\n\n\n\n\n\n\n\n\n\n\n\nNull models\nnull model to compare with fitted models\n\nGLM\n\nextractAIC(null.glm)[2]\n\n[1] 294891.4\n\n\n\n\nGWPR\n\n# AICc for null GWR model\nnull.gwpr$glms$aic\n\n[1] 293493.6\n\n\n\n\nBayesian Negative binomial",
    "crumbs": [
      "Modelling",
      "Model Evaluation"
    ]
  }
]
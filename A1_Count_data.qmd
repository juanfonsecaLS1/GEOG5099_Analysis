---
title: "Count Data"
author: "Juan Fonseca"
format: html
---

```{r}
library(tidyverse)
library(rjson)
library(httr)
library(sf)
library(tmap)
```
### Sensor summary

The following code loads the file with the summary of scoot loop sensors in Hull

```{r}
Cam_Locations  <- GET("https://opendata.hullcc.gov.uk/dataset/30fd3969-556d-4eae-ae4c-f3f9d2cfa9e3/resource/90e1cce0-295e-4fa7-aa21-ebc4f3e8e8d4/download/scoot_loop_resources_full.json")

my_response <- rjson::fromJSON(content(Cam_Locations,'text',encoding = "UTF-8"))

my_data <- data.frame(do.call(rbind,
                             lapply(my_response,
                                    rbind))) |>
  unnest(cols = everything()) |>
  filter(longitude != 0)
```

A spatial object is created using the coordinates of the sites

```{r}
sf_cameras <- my_data |> 
  st_as_sf(
    coords = c("longitude","latitude"),
    crs = 4326) |>
  st_transform(crs = 27700)
```

```{r}
tm_basemap() +
		tm_shape(sf_cameras) +
		tm_dots()
```
There are some issues with the coordinates of some sites. The following code fixes the problem

```{r}
my_data[my_data$latitude<0,c("longitude","latitude")] <- rev(my_data[my_data$latitude<0,c("longitude","latitude")])
```

The `description` column contain useful information on the location of each sensor, this can be used to identify sensors corresponding to different lanes in the same road, for example.

```{r}
my_data_expanded <- my_data |>
  mutate(description = str_replace(description,"HE SITE - ","HE SITE _ ")) |> 
  separate_wider_delim(description,delim = " - ",names = c("desc","direction","url","coord")) |>
  select(-coord)
```


```{r}
sf_sensors_raw <- my_data_expanded |> 
  st_as_sf(
    coords = c("longitude","latitude"),
    crs = 4326) |>
  st_transform(crs = 27700)

tm_basemap() +
		tm_shape(sf_sensors_raw) +
		tm_dots()
```

#### Grouping sensors

```{r}
sensors_buffer_5 = sf_sensors_raw |> st_buffer(dist = 5)
```

```{r}
tm_basemap() +
  tm_shape(sensors_buffer_5 |> 
             filter(name %in% c("N44131F","N44131X","N44131H"))) +
		tm_polygons(alpha = 0.4)+
		tm_shape(sf_sensors_raw) +
		tm_dots()
```

```{r}
sensors_overlap = st_intersects(sensors_buffer_5,sensors_buffer_5) |> unique()
head(sensors_overlap)
```
```{r}
dir_groups <- do.call(bind_rows,
                      lapply(seq_along(sensors_overlap),
                             function(i){
                               tsensor <-  sensors_overlap[[i]]
                               id_group  <- i
                               tmp_group <- my_data_expanded[tsensor,] |>
                                 mutate(subgroup_id = cur_group_id(),
                                        group_id = id_group,
                                        .by = c(desc,direction)) |>
                                 select(name,desc,direction,group_id,subgroup_id)
                               })) 
```

```{r}
sf_counts <- my_data_expanded |> 
  left_join(dir_groups,by = join_by(name,desc,direction)) |> 
  summarise(across(ends_with("itude"),mean),
            .by = c(group_id,subgroup_id,desc,direction)) |> 
  st_as_sf(
    coords = c("longitude","latitude"),
    crs = 4326) |>
  st_transform(crs = 27700)
```

```{r}
tmap_mode("plot")
tm_basemap()+
  tm_shape(sf_counts) +
  tm_dots()
```

### Downloading data

Since the open data platform hosting the counts data is based on CKAN, the raw CSV files can be downloaded directly with the following code (see [this](https://docs.ckan.org/en/latest/maintaining/datastore.html#downloading-resources)):

```{r}
dir.create("02_raw_data_counts", showWarnings = F)

base_url <- "https://opendata.hullcc.gov.uk/datastore/dump/"
base_path <- "02_raw_data_counts"
```

```{r}
# In case files are downloaded separately 
downloaded_files <- list.files("02_raw_data_counts/")
ids_download <- my_data$resource_id[!(my_data$name %in% gsub("\\.csv","",x = downloaded_files))]

# Loop for downloading files
if (!identical(ids_download, character(0))) {
  for (id in ids_download) {
    try(download.file(
      url = paste0(base_url, id),
      destfile = paste0(base_path,
                        "/",
                        my_data$name[my_data$resource_id == id],
                        ".csv")
    ))
  }
} 
```

### Pre-processing

First, we upload the `downloaded_files` object with all the files available
```{r}
downloaded_files_full <- list.files("02_raw_data_counts/",full.names = T)
```

```{r}
library(data.table)
library(dtplyr)
setDTthreads(0)

raw_data <- rbindlist(lapply(downloaded_files_full,fread))
head(raw_data)
```
Some general checks of the data

Number of sites in the raw data:
```{r}
raw_data$LinkID |> unique() |> length()
```
Range of dates:
```{r}
raw_data$MeasurementTime |> range()
```
#### Collection rate check
A check of the number of records per site
```{r}
hist(raw_data |>
       summarise(records = n(),.by = LinkID) |>
       as_tibble() |>
       pull(records),breaks = seq(0,350000,1000),
     xlab = "Number of records",
     main = "Distribution of total records per LinkID")

```
A closer look to the sites with a low number of records:
```{r}
low_records_IDs <- raw_data |>
  filter(between(year(MeasurementTime),2022,2023)) |> 
  summarise(records = n(),.by = LinkID) |>
  filter(records < 150000) |> 
  as_tibble() |> 
  pull(LinkID)

low_records_IDs
```

```{r}
tmap_mode("view")
tm_basemap()+
  tm_shape(sf_sensors_raw |>
  filter(!(name %in% low_records_IDs)))+
  tm_dots(col = "black",
          alpha = 0.3,
          group = "Other sites")+
  tm_shape(sf_sensors_raw |>
             filter(name %in% low_records_IDs))+
  tm_dots(col = "red",
          group = "Low record sites")
```

```{r}
daily_data <- raw_data[between(year(MeasurementTime),2022,2023)
             ][,GBtimestamp := with_tz(MeasurementTime,tzone = "Europe/London")
               ][,`:=`(Date = date(GBtimestamp),
                       Year = year(GBtimestamp))][
                         ,.(records = .N,
                            Flow = sum(VehicleFlow,na.rm = T)),
                         by = .(LinkID, Date, Year)
                       ]
```

Records per day per site by Year (max 288 5-minutes intervals)
```{r}
daily_data[,.(records = mean(records)),
  .(LinkID, Year)] |> 
ggplot(aes(records))+
  geom_histogram(binwidth = 1,col = "white")+
  facet_grid(Year~.)+
  theme_light()
```
To identify the IDs with a low number of daily records, we run:
```{r}
low_annual_IDs <- unique(daily_data[,c("Year","LinkID","Date")])[,
                                               .(n_days = .N),
                                               .(LinkID, Year)][n_days<240] |> pull(LinkID)
```

The IDs with low records match exactly the ones previously identified.
```{r}
identical(low_annual_IDs,low_records_IDs)
```
These IDs will be discarded as the available data might not be representative to produce AADF; if the ID is part of a group, the whole group will be discarded as AADF for the group can be affected. 

```{r}
groups_include <- dir_groups |> 
  mutate(not_include = (name %in% low_annual_IDs)*1) |> 
  filter(sum(not_include)==0,.by = c(desc,direction,group_id)) |>
  select(-not_include) 
```

#### Daily flows sense check

```{r}
total_d_flow <- daily_data |> 
  inner_join(groups_include,by = c("LinkID"="name")) |> 
  summarise(Flow = sum(Flow),.by=Date)


  ggplot(total_d_flow,
         aes(x=Date,y=Flow))+
  geom_line() +
  geom_point(data = total_d_flow |>
               filter (Flow<500e3),
             col = "red")+
  geom_text(data = total_d_flow |>
               filter (Flow<500e3),
            aes(label = Date),
             col = "#020202")

```
The daily flows on Christmas day are used a sensible threshold to identify outliers which will be discarded. It is assumed that the lowest traffic over the year occurs on that day.
```{r}
min_Xmas <- daily_data |>
  filter(day(Date)==25,month(Date)==12) |> 
  summarise(Flow = sum(Flow),.by=c(Date)) |> 
  pull(Flow) |> 
  min()

total_d_flow <- daily_data |> 
  inner_join(groups_include,by = c("LinkID"="name")) |> 
  filter(sum(Flow)>=(min_Xmas*0.99),.by=Date) |> 
  summarise(Flow = sum(Flow),.by=Date)


  ggplot(total_d_flow,
         aes(x=Date,y=Flow))+
  geom_line() +
  geom_point(data = total_d_flow |>
               filter (Flow<600e3),
             col = "red")+
  geom_text(data = total_d_flow |>
               filter (Flow<600e3),
            aes(label = Date),
             col = "#020202")

```

#### Zero-flow sites check

```{r}
site_d_flow <- daily_data |> 
  inner_join(groups_include,by = c("LinkID"="name")) |> 
  filter(sum(Flow)>=min_Xmas,.by=Date) |> 
  summarise(Flow = sum(Flow),.by=c(Year,Date,group_id,subgroup_id)) 

no_flow_sites <- site_d_flow |>
  summarise(Flow = sum(Flow),.by=c(group_id,subgroup_id)) |>
  filter(Flow == 0)

```

```{r}
tmap_mode("view")
tm_basemap()+
  tm_shape(sf_counts |>
             anti_join(no_flow_sites,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "black",alpha = 0.3,group = "Other sites")+
  tm_shape(sf_counts |>
             semi_join(no_flow_sites,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "red",group = "Zero counts sites")
```



#### Annual Average Daily Flow (AADF) calculation 
```{r}
aadf_data <- site_d_flow |> 
  anti_join(no_flow_sites,
                       by = join_by(group_id,subgroup_id)) |> 
  summarise(Flow = mean(Flow),.by = c(Year,group_id,subgroup_id)) |> 
  pivot_wider(names_from = "Year",values_from = "Flow",names_prefix = "flow.")
```

#### 2022 vs 2023 flows high-level check

```{r}
aadf_checks <- aadf_data |> 
  mutate(diff = flow.2023-flow.2022,
         pdiff = diff/flow.2022) |> 
  arrange(-abs(pdiff))
```

```{r}
  ggplot(data = aadf_checks,
         aes(flow.2022,flow.2023))+
  geom_smooth(formula = "y ~ x+0",method = "lm",se = F,alpha = 0.4)+
  geom_point(shape = 19,alpha = 0.6)+
  geom_point(data = aadf_checks |>
               filter(abs(pdiff)>0.5|is.nan(pdiff)),
             shape = 19,
             alpha = 0.4,
             size = 3,
             col= "red")+
  theme_light()+
  coord_fixed()
```
```{r}
high_change_counts <- aadf_checks |>
  filter(abs(pdiff)>0.5|is.nan(pdiff))
```

```{r}
tm_basemap()+
  tm_shape(sf_counts |>
             anti_join(high_change_counts,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "black",alpha = 0.3,group = "Other sites")+
  tm_shape(sf_counts |>
             semi_join(high_change_counts,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "red",group = "High change sites")
```


Although these changes seem suspicious, these records will not be discarded for the final analysis.

### Saving results

The spatial object with the counts is updated to be consistent with the AADF dataframe

```{r}
sf_counts_clean <- sf_counts |>
  semi_join(aadf_data,
            by = join_by(group_id,subgroup_id))
```

The following code produces a `csv` file with the AADF of all sites, and a `geoJSON` file for the .
```{r}
dir.create("03_preprocessing_files",showWarnings = F)
write_csv(aadf_data,file = "03_preprocessing_files/aadf_data.csv",append = F)
try(file.remove("03_preprocessing_files/grouped_counts.geojson"))
st_write(sf_counts_clean,"03_preprocessing_files/grouped_counts.geojson",append = F)
```
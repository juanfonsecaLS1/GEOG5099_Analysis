---
title: "Count Data"
author: "Juan Fonseca"
format: html
---

```{r}
library(tidyverse)
library(rjson)
library(httr)
library(sf)
library(tmap)
```

### Sensor summary

The following code loads the file with the summary of scoot loop sensors in Hull

```{r load_sensor_locations}
loop_Locations  <- GET("https://opendata.hullcc.gov.uk/dataset/30fd3969-556d-4eae-ae4c-f3f9d2cfa9e3/resource/90e1cce0-295e-4fa7-aa21-ebc4f3e8e8d4/download/scoot_loop_resources_full.json")

my_response <- rjson::fromJSON(content(loop_Locations,'text',encoding = "UTF-8"))

my_data <- data.frame(do.call(rbind,
                             lapply(my_response,
                                    rbind))) |>
  unnest(cols = everything()) |>
  filter(longitude != 0)
```

A spatial object is created using the coordinates of the sites

```{r sf_transform_1}
sf_cameras <- my_data |> 
  st_as_sf(
    coords = c("longitude","latitude"),
    crs = 4326) |>
  st_transform(crs = 27700)
```

```{r all_loop_map}
tm_basemap() +
		tm_shape(sf_cameras) +
		tm_dots()
```

There are some issues with the coordinates of some sites. The following code fixes the problem

```{r coords_fixing}
my_data[my_data$latitude<0,c("longitude","latitude")] <- rev(my_data[my_data$latitude<0,c("longitude","latitude")])
```

The `description` column contain useful information on the location of each sensor, this can be used to identify sensors corresponding to different lanes in the same road, for example.

```{r dir_extract}
my_data_expanded <- my_data |>
  mutate(description = str_replace(description,"HE SITE - ","HE SITE _ ")) |> 
  separate_wider_delim(description,delim = " - ",names = c("desc","direction","url","coord")) |>
  select(-coord)
```

```{r map_coord_fixed}
sf_sensors_raw <- my_data_expanded |> 
  st_as_sf(
    coords = c("longitude","latitude"),
    crs = 4326) |>
  st_transform(crs = 27700)

tm_basemap("OpenStreetMap") +
		tm_shape(sf_sensors_raw) +
		tm_dots()
```

#### Grouping sensors

```{r}
sensors_buffer = sf_sensors_raw |>
  st_buffer(dist = 20)
```

```{r map_sites_combine_sample}
tm_basemap("OpenStreetMap") +
  tm_shape(sensors_buffer |> 
             filter(name %in% c("N44131F","N44131X","N44131H"))) +
		tm_polygons(alpha = 0.4)+
		tm_shape(sf_sensors_raw) +
		tm_dots()
```

```{r overlap_list}
buffer_groups  <- st_cast(st_union(sensors_buffer),"POLYGON")
sensors_overlap <- st_intersects(buffer_groups,sf_sensors_raw)
sensors_overlap
```

#### Flow Direction refining

The description of the count sites refers to the junction where the induction loops are installed, and not necessarily correspond to actual road where they are installed.

The sites shown in the map below, for example, are all described as "TSI047 COUNTY ROAD / BRICKNALL AVE", but they correspond to the four arms of the junction with sensors on Fairfax Ave, Bricknell Ave, and National Ave.

```{r dir_extract1}
tmap_mode("view")
tm_basemap("OpenStreetMap")+
  tm_shape(sf_sensors_raw |>
             filter(str_detect(desc,"TSI047 COUNTY ROAD / BRICKNALL AVE")))+
  tm_dots(col = "darkgreen",
          size = 2,
          alpha = 0.7,
          group = "Selected sites")
```

Also, the direction of the different counts, in some cases, refer to the specific lane where the sensors are installed. Therefore, it is useful to extract the flow direction from description.

Some count points need to be adjusted manually:
- there is no a valid description of the direction of sensor `N43121Y`. See [this](https://maps.app.goo.gl/3rBHcqJooss6Z5Zp9). 
- there is a typo (*wesbound* instead of *wes**t**bound*) in the direction of sensors `N40134F`and `N45131D`.

```{r dir_manual_fixes}
my_data_expanded$direction[my_data_expanded$name == "N43121Y"] <- "EASTBOUND"
my_data_expanded$direction[my_data_expanded$name %in% c("N40134F", "N45131D")] <- "WESTBOUND"
```

```{r sf_clean2}
my_data_expanded_dir <- my_data_expanded |> 
  mutate(dir_str = str_to_lower(direction) |> 
           str_extract(pattern = "\\b\\w*bound\\b")) |> 
  mutate(origin_str = if_else(is.na(dir_str),
                           direction |> 
                             str_remove("TRAVEL{1,3}ING ") |>
                             str_remove("\\sINTENDING") |>
                             str_extract("\\w*(\\s)?\\w*\\b(?=\\sTO)"),
                           NA)) |> 
  mutate(dir_str = case_when(is.na(dir_str)&origin_str=="EAST"~"westbound",
                             is.na(dir_str)&origin_str=="NORTH"~"southbound",
                             is.na(dir_str)&origin_str=="SOUTH WEST"~"northeastbound",
                             is.na(dir_str)&origin_str=="SOUTH"~"northbound",
                             is.na(dir_str)&origin_str=="WEST"~"eastbound",
                             is.na(dir_str)&origin_str=="SOUTH NORTH"~"northbound",
                             is.na(dir_str)&str_detect(direction,"FROM THE NORTH")~"southbound",
                             is.na(dir_str)&str_detect(direction,"FROM THE SOUTH")~"northbound",
                             is.na(dir_str)&str_detect(direction,"FROM THE EAST")~"westbound",
                             is.na(dir_str)&str_detect(direction,"FROM THE WEST")~"eastbound",
                             TRUE ~ dir_str)) |> 
  select(-origin_str)
```

```{r sensor_groups}
dir_groups <- do.call(bind_rows,
                      lapply(seq_along(sensors_overlap),
                             function(i){
                               tsensor <-  sensors_overlap[[i]]
                               id_group  <- i
                               tmp_group <- my_data_expanded_dir[tsensor,] |>
                                 mutate(subgroup_id = cur_group_id(),
                                        group_id = id_group,
                                        .by = c(desc,dir_str)) |>
                                 select(name,desc,dir_str,group_id,subgroup_id)
                               })) 
```

```{r grouped_sf}
sf_counts <- my_data_expanded_dir |> 
  left_join(dir_groups,by = join_by(name,desc,dir_str)) |> 
  summarise(across(ends_with("itude"),mean),
            .by = c(group_id,subgroup_id,desc,dir_str)) |> 
  st_as_sf(
    coords = c("longitude","latitude"),
    crs = 4326) |>
  st_transform(crs = 27700)
```

```{r grouped_sensors_map}
tmap_mode("plot")
tm_basemap("OpenStreetMap")+
  tm_shape(sf_counts) +
  tm_dots()
```

### Downloading data

Since the open data platform hosting the counts data is based on CKAN, the raw CSV files can be downloaded directly with the following code (see [this](https://docs.ckan.org/en/latest/maintaining/datastore.html#downloading-resources)):

```{r count_data_pars}
dir.create("02_raw_data_counts", showWarnings = F)

base_url <- "https://opendata.hullcc.gov.uk/datastore/dump/"
base_path <- "02_raw_data_counts"
```

```{r count_data_fetch}
# In case files are downloaded separately 
downloaded_files <- list.files("02_raw_data_counts/")
ids_download <- my_data$resource_id[!(my_data$name %in% gsub("\\.csv","",x = downloaded_files))]

# Loop for downloading files
if (!identical(ids_download, character(0))) {
  for (id in ids_download) {
    try(download.file(
      url = paste0(base_url, id),
      destfile = paste0(base_path,
                        "/",
                        my_data$name[my_data$resource_id == id],
                        ".csv")
    ))
  }
} 
```

### Pre-processing

First, we upload the `downloaded_files` object with all the files available

```{r count_data_files}
downloaded_files_full <- list.files("02_raw_data_counts/",full.names = T)
```

```{r count_data_load}
library(data.table)
library(dtplyr)
setDTthreads(0)

raw_data <- rbindlist(lapply(downloaded_files_full,fread))
head(raw_data)
```

Some general checks of the data

Number of sites in the raw data:

```{r check1}
raw_data$LinkID |> unique() |> length()
```

Range of dates:

```{r check_dates}
raw_data$MeasurementTime |> range()
```

#### Collection rate check

A check of the number of records per site

```{r check2}
hist(raw_data |>
       summarise(records = n(),.by = LinkID) |>
       as_tibble() |>
       pull(records),breaks = seq(0,350000,1000),
     xlab = "Number of records",
     main = "Distribution of total records per LinkID")

```

A closer look to the sites with a low number of records:

```{r check3}
low_records_IDs <- raw_data |>
  filter(between(year(MeasurementTime),2022,2023)) |> 
  summarise(records = n(),.by = LinkID) |>
  filter(records < 150000) |> 
  as_tibble() |> 
  pull(LinkID)

low_records_IDs
```

```{r map_check3}
tmap_mode("plot")
tm_basemap()+
  tm_shape(sf_sensors_raw |>
  filter(!(name %in% low_records_IDs)))+
  tm_dots(col = "black",
          alpha = 0.3,
          group = "Other sites")+
  tm_shape(sf_sensors_raw |>
             filter(name %in% low_records_IDs))+
  tm_dots(col = "red",
          group = "Low record sites")
```

```{r check4a}
daily_data <- raw_data[between(year(MeasurementTime),2022,2023)
             ][,GBtimestamp := with_tz(MeasurementTime,tzone = "Europe/London")
               ][,`:=`(Date = date(GBtimestamp),
                       Year = year(GBtimestamp))][
                         ,.(records = .N,
                            Flow = sum(VehicleFlow,na.rm = T)),
                         by = .(LinkID, Date, Year)
                       ]
```

Records per day per site by Year (max 288 5-minutes intervals)

```{r check4a_plot}
daily_data[,.(records = mean(records)),
  .(LinkID, Year)] |> 
ggplot(aes(records))+
  geom_histogram(binwidth = 1,col = "white")+
  facet_grid(Year~.)+
  theme_light()
```

To identify the IDs with a low number of daily records, we run:

```{r check5}
low_annual_IDs <- unique(daily_data[,c("Year","LinkID","Date")])[,
                                               .(n_days = .N),
                                               .(LinkID, Year)][n_days<240] |> pull(LinkID)
```

The IDs with low records match exactly the ones previously identified.

```{r check5_check4_comp}
identical(low_annual_IDs,low_records_IDs)
```

These IDs will be discarded as the available data might not be representative to produce AADF; if the ID is part of a group, the whole group will be discarded as AADF for the group can be affected.

```{r data_clean}
groups_include <- dir_groups |> 
  mutate(not_include = (name %in% low_annual_IDs)*1) |> 
  filter(sum(not_include)==0,.by = c(desc,dir_str,group_id)) |>
  select(-not_include) 
```

#### Daily flows sense check

```{r check6_plot}
total_d_flow <- daily_data |> 
  inner_join(groups_include,by = c("LinkID"="name")) |> 
  summarise(Flow = sum(Flow),.by=Date)


  ggplot(total_d_flow,
         aes(x=Date,y=Flow))+
  geom_line() +
  geom_point(data = total_d_flow |>
               filter (Flow<500e3),
             col = "red")+
  geom_text(data = total_d_flow |>
               filter (Flow<500e3),
            aes(label = Date),
             col = "#020202")

```

The daily flows on Christmas day are used a sensible threshold to identify outliers which will be discarded. It is assumed that the lowest traffic over the year occurs on that day.

```{r check6_clean}
min_Xmas <- daily_data |>
  filter(day(Date)==25,month(Date)==12) |> 
  summarise(Flow = sum(Flow),.by=c(Date)) |> 
  pull(Flow) |> 
  min()

total_d_flow <- daily_data |> 
  inner_join(groups_include,by = c("LinkID"="name")) |> 
  filter(sum(Flow)>=(min_Xmas*0.99),.by=Date) |> 
  summarise(Flow = sum(Flow),.by=Date)


  ggplot(total_d_flow,
         aes(x=Date,y=Flow))+
  geom_line() +
  geom_point(data = total_d_flow |>
               filter (Flow<600e3),
             col = "red")+
  geom_text(data = total_d_flow |>
               filter (Flow<600e3),
            aes(label = Date),
             col = "#020202")

```

#### Zero-flow sites check

```{r check7}
site_d_flow <- daily_data |> 
  inner_join(groups_include,by = c("LinkID"="name")) |> 
  filter(sum(Flow)>=min_Xmas,.by=Date) |> 
  summarise(Flow = sum(Flow),.by=c(Year,Date,group_id,subgroup_id)) 

no_flow_sites <- site_d_flow |>
  summarise(Flow = sum(Flow),.by=c(group_id,subgroup_id)) |>
  filter(Flow == 0)

```

```{r check7_map}
tmap_mode("plot")
tm_basemap()+
  tm_shape(sf_counts |>
             anti_join(no_flow_sites,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "black",alpha = 0.3,group = "Other sites")+
  tm_shape(sf_counts |>
             semi_join(no_flow_sites,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "red",group = "Zero counts sites")
```

#### Annual Average Daily Flow (AADF) calculation

```{r aadf_calc}
aadf_data <- site_d_flow |> 
  anti_join(no_flow_sites,
                       by = join_by(group_id,subgroup_id)) |> 
  summarise(Flow = mean(Flow),.by = c(Year,group_id,subgroup_id)) |> 
  pivot_wider(names_from = "Year",values_from = "Flow",names_prefix = "flow.")
```

#### 2022 vs 2023 flows high-level check

```{r check8}
aadf_checks <- aadf_data |> 
  mutate(diff = flow.2023-flow.2022,
         pdiff = diff/flow.2022) |> 
  arrange(-abs(pdiff))
```

```{r check8_lmplot}
  ggplot(data = aadf_checks,
         aes(flow.2022,flow.2023))+
  geom_smooth(formula = "y ~ x+0",method = "lm",se = F,alpha = 0.4)+
  geom_point(shape = 19,alpha = 0.6)+
  geom_point(data = aadf_checks |>
               filter(abs(pdiff)>0.5|is.nan(pdiff)),
             shape = 19,
             alpha = 0.4,
             size = 3,
             col= "red")+
  theme_light()+
  coord_fixed()
```

```{r check8a}
high_change_counts <- aadf_checks |>
  filter(abs(pdiff)>0.5|is.nan(pdiff))
```

```{r check8_map}
tmap_mode("plot")
tm_basemap()+
  tm_shape(sf_counts |>
             anti_join(high_change_counts,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "black",alpha = 0.3,group = "Other sites")+
  tm_shape(sf_counts |>
             semi_join(high_change_counts,
                       by = join_by(group_id,subgroup_id)))+
  tm_dots(col = "red",group = "High change sites")
```

Although these changes seem suspicious, these records will not be discarded for the final analysis.

The spatial object with the counts is updated to be consistent with the AADF dataframe

```{r sf_clening}
sf_counts_selected <- sf_counts |>
  semi_join(aadf_data,
            by = join_by(group_id,subgroup_id))
```

### Saving results

The following code produces a `csv` file with the AADF of all sites, and a `geoJSON` file for the .

```{r count_data_saving}
dir.create("03_preprocessing_files",showWarnings = F)
write_csv(aadf_data,file = "03_preprocessing_files/aadf_data.csv",append = F)
try(file.remove("03_preprocessing_files/grouped_counts.geojson"))
st_write(sf_counts_selected,"03_preprocessing_files/grouped_counts.geojson",append = F)
```
